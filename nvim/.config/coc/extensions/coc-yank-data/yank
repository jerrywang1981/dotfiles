a782b3c37d06d36035ec75531c29eac4|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill_service.py|189|1|V|python
	            print("\t1. clean up all tmp table and local files")
fc99d881d0d1af15e6c62f5564ba07e4|file:///Users/jerry/repo/ibmeww-etl/script/common/base_loader.py|1|1|V|python
	    @contextmanager
501ac4041be925a7687466dd2570be0c|file:///Users/jerry/repo/ibmeww-etl/script/common/base_loader.py|1|1|V|python
	    @contextmanager
	    def run_query_context(self, commit=True):
7ea49a84c090457c3943dd6e309e958f|file:///Users/jerry/repo/ibmeww-etl/script/common/base_loader.py|1|1|V|python
	    @contextmanager
	    def run_query_context(self, commit=True):
	        '''the context manager to run a query
ae0fb4a39739dc3cf64306ed80f6af74|file:///Users/jerry/repo/ibmeww-etl/script/common/base_loader.py|1|1|V|python
	    @contextmanager
	    def run_query_context(self, commit=True):
	        '''the context manager to run a query
	
3cc982bb3509ead44c77a8b958578ce0|file:///Users/jerry/repo/ibmeww-etl/script/common/base_loader.py|1|1|V|python
	    @contextmanager
	    def run_query_context(self, commit=True):
	        '''the context manager to run a query
	
	        Keyword arguments:
1c9e2afe9563e4582c7e4a103fa0136d|file:///Users/jerry/repo/ibmeww-etl/script/common/base_loader.py|1|1|V|python
	    @contextmanager
	    def run_query_context(self, commit=True):
	        '''the context manager to run a query
	
	        Keyword arguments:
	            commit -- to commit your query or not, default to True
2ab556d743a1c4fee8f6c0128cc8612a|file:///Users/jerry/repo/ibmeww-etl/script/common/base_loader.py|1|1|V|python
	    @contextmanager
	    def run_query_context(self, commit=True):
	        '''the context manager to run a query
	
	        Keyword arguments:
	            commit -- to commit your query or not, default to True
	
07da4111040d9d9d5fb822e066fff7dc|file:///Users/jerry/repo/ibmeww-etl/script/common/base_loader.py|1|1|V|python
	    @contextmanager
	    def run_query_context(self, commit=True):
	        '''the context manager to run a query
	
	        Keyword arguments:
	            commit -- to commit your query or not, default to True
	
	        by default it will commit your query
6ad62d6ecb64b58d93d4280df86bc1f7|file:///Users/jerry/repo/ibmeww-etl/script/common/base_loader.py|1|1|V|python
	    @contextmanager
	    def run_query_context(self, commit=True):
	        '''the context manager to run a query
	
	        Keyword arguments:
	            commit -- to commit your query or not, default to True
	
	        by default it will commit your query
	        '''
2c5a17710e53b50658ab6b74d541b325|file:///Users/jerry/repo/ibmeww-etl/script/common/base_loader.py|1|1|V|python
	    @contextmanager
	    def run_query_context(self, commit=True):
	        '''the context manager to run a query
	
	        Keyword arguments:
	            commit -- to commit your query or not, default to True
	
	        by default it will commit your query
	        '''
	        try:
41c02b66fe23f5d39ddbeb300cf579a8|file:///Users/jerry/repo/ibmeww-etl/script/common/base_loader.py|1|1|V|python
	    @contextmanager
	    def run_query_context(self, commit=True):
	        '''the context manager to run a query
	
	        Keyword arguments:
	            commit -- to commit your query or not, default to True
	
	        by default it will commit your query
	        '''
	        try:
	            yield
3c5065caa074755156e00fddda512ba8|file:///Users/jerry/repo/ibmeww-etl/script/common/base_loader.py|1|1|V|python
	    @contextmanager
	    def run_query_context(self, commit=True):
	        '''the context manager to run a query
	
	        Keyword arguments:
	            commit -- to commit your query or not, default to True
	
	        by default it will commit your query
	        '''
	        try:
	            yield
	        except Exception as e:
f42e3679821c0ed6ef4449d529625145|file:///Users/jerry/repo/ibmeww-etl/script/common/base_loader.py|1|1|V|python
	    @contextmanager
	    def run_query_context(self, commit=True):
	        '''the context manager to run a query
	
	        Keyword arguments:
	            commit -- to commit your query or not, default to True
	
	        by default it will commit your query
	        '''
	        try:
	            yield
	        except Exception as e:
	            traceback.print_exc()
6ee4dad270f02fa0b9eb950029420d4d|file:///Users/jerry/repo/ibmeww-etl/script/common/base_loader.py|1|1|V|python
	    @contextmanager
	    def run_query_context(self, commit=True):
	        '''the context manager to run a query
	
	        Keyword arguments:
	            commit -- to commit your query or not, default to True
	
	        by default it will commit your query
	        '''
	        try:
	            yield
	        except Exception as e:
	            traceback.print_exc()
	            self.pgsql.rollback()
0e9c721466fecb005e2102eab028866d|file:///Users/jerry/repo/ibmeww-etl/script/common/base_loader.py|1|1|V|python
	    @contextmanager
	    def run_query_context(self, commit=True):
	        '''the context manager to run a query
	
	        Keyword arguments:
	            commit -- to commit your query or not, default to True
	
	        by default it will commit your query
	        '''
	        try:
	            yield
	        except Exception as e:
	            traceback.print_exc()
	            self.pgsql.rollback()
	            raise gip_exception.GIPException(e)
1d7948da33545d8b5d52c4a79ad980a7|file:///Users/jerry/repo/ibmeww-etl/script/common/base_loader.py|1|1|V|python
	    @contextmanager
	    def run_query_context(self, commit=True):
	        '''the context manager to run a query
	
	        Keyword arguments:
	            commit -- to commit your query or not, default to True
	
	        by default it will commit your query
	        '''
	        try:
	            yield
	        except Exception as e:
	            traceback.print_exc()
	            self.pgsql.rollback()
	            raise gip_exception.GIPException(e)
	        finally:
ca9d63dc818809ebeba6a9bcc631a5e8|file:///Users/jerry/repo/ibmeww-etl/script/common/base_loader.py|1|1|V|python
	    @contextmanager
	    def run_query_context(self, commit=True):
	        '''the context manager to run a query
	
	        Keyword arguments:
	            commit -- to commit your query or not, default to True
	
	        by default it will commit your query
	        '''
	        try:
	            yield
	        except Exception as e:
	            traceback.print_exc()
	            self.pgsql.rollback()
	            raise gip_exception.GIPException(e)
	        finally:
	            if commit:
be67e38d408e67556de0a6c74c08f032|file:///Users/jerry/repo/ibmeww-etl/script/common/base_loader.py|1|1|V|python
	    @contextmanager
	    def run_query_context(self, commit=True):
	        '''the context manager to run a query
	
	        Keyword arguments:
	            commit -- to commit your query or not, default to True
	
	        by default it will commit your query
	        '''
	        try:
	            yield
	        except Exception as e:
	            traceback.print_exc()
	            self.pgsql.rollback()
	            raise gip_exception.GIPException(e)
	        finally:
	            if commit:
	                self.pgsql.commit()
e39eb3769394a4a8ea14d77515296aaa|file:///Users/jerry/repo/ibmeww-etl/script/common/base_loader.py|1|1|V|python
	    @contextmanager
	    def run_query_context(self, commit=True):
	        '''the context manager to run a query
	
	        Keyword arguments:
	            commit -- to commit your query or not, default to True
	
	        by default it will commit your query
	        '''
	        try:
	            yield
	        except Exception as e:
	            traceback.print_exc()
	            self.pgsql.rollback()
	            raise gip_exception.GIPException(e)
	        finally:
	            if commit:
	                self.pgsql.commit()
	
615b754193f34ceb37d34d8a29cf8441|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill_service_dal.py|52|1|V|python
	    def __clean_table_commit(self,schema,table):
	        sql='delete from {0}.{1} where 1=1'.format(schema,table)
	        self.pgsql.query(sql, reply=False)
	        self.pgsql.commit()
5c6b2e3d8cf899ef7dc4fbe25de09fd4|file:///Users/jerry/repo/ibmeww-etl/script/surface/surface_service.py|17|1|v|python
	INDEX_TABLE
103ece273490fa48527a91c82367644c|file:///Users/jerry/repo/ibmeww-etl/script/surface/surface_service.py|41|1|V|python
	        super().clean_tmp_tables(self.tmp_schema, INDEX_TABLE)
0be4401838c5afe62ea59a2d1be697a6|file:///Users/jerry/repo/ibmeww-etl/script/surface/surface_service.py|18|1|v|python
	DATA_TABLE
99c7f7a03bf1d332d111e37335b35c65|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill_service.py|196|1|V|python
	            #  print("\t2. download files and load into tmp table")
85a8196103cf060c84df6f31b81906f0|file:///Users/jerry/repo/ibmeww-etl/script/surface/surface_service.py|48|1|V|python
	        print(upload_files)
	        logger.info("verify files to be loaded into db ")
1b2a5b499ae5c7548d0206ea6dca0def|file:///Users/jerry/repo/ibmeww-etl/script/surface/surface_service.py|48|1|V|python
	        print(upload_files)
81262e82c810a5b2a0ac6fcc4038ad3d|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill_service.py|276|1|V|python
	if __name__ == '__main__':
	    conf = config_handler.ConfigHandler()
c8ed21734a8f706d840666bf7711ecab|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill_service.py|19|1|V|python
	import sys
	sys.path.append(path.dirname( path.dirname( path.dirname( path.abspath(__file__) ))) )
5dc84632a62076dee815898d913eaa4a|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill_service.py|197|1|V|python
	            #  for file in upload_files:
	                #  if file =='.DS_Store' or (not file.endswith(('.txt', '.csv'))) or(file.lower().find('chip')>=0):#ignore the error file
	                    #  continue
	                #  file_path=self.download_filepath
	                #  if self.file_on_ObjectStorage:
	                    #  filename_local = file.replace('/', ':')
	                    #  coshandler.download_file(self.target_bucket, self.download_filepath, file, filename_local)
	                #  else:
	                    #  filename_local=file
	                #  print(filename_local)
	                #  drill.DrillLoader(file_path + filename_local, self.pgsql).load()
	                #  os.remove(self.download_filepath+filename_local)
	                #  if self.file_on_ObjectStorage:
	                    #  backup_name = str(datetime.datetime.now()) + '_' + file
	                    #  coshandler.move_file(self.target_bucket, self.backup_bucket, file, backup_name)
c3c5ba019344640f32a638f31609ad2b|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill_service.py|167|1|V|python
	        try:
	            if self.file_on_ObjectStorage:
	                print("get access to object storage")
	                self.error_bucket = self.conf.get_error_bucket()
	                self.backup_bucket = self.conf.get_backup_bucket()
	                self.target_bucket = self.conf.get_target_bucket(self.task)
	                cos_conf = self.conf.get_cos_conf()
	                coshandler = cos_handler.COSHandler(cos_conf)
	                # check whether error_bucket bucket exists
	                self.check_all_bucket(coshandler)
	                upload_files = coshandler.list_files(self.target_bucket, fileroot)
	            else:
	                upload_files=os.listdir(self.download_filepath)
	            print("verify files to be loaded into db ")
	            self.validate_uploadfiles_batch(upload_files)
	        except Exception as e:
	            traceback.print_exc()
	            raise gip_exception.GIPException(e)
c7cd9d4592b19166e2345ccc70c4c0ce|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill_service.py|23|1|V|python
	import script.common.cos_handler as cos_handler
e845aadf22d62ed72d957bb1a9f932e3|file:///Users/jerry/repo/ibmeww-etl/script/geointerval/dataAccessLayer.py|172|10|v|python
	pointstructure_index,
9d4074c0b82f981b9fb8574736d44749|file:///Users/jerry/repo/ibmeww-etl/script/geointerval/dataAccessLayer.py|96|1|V|python
	            col = 'holeid,projectid,fromid,toid,'+ dic_intervalid[table_1]
4754c9e776ee84bf471fa9f642a9f660|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill_service_dal.py|211|1|V|python
	        self.pgsql.query(cmd=sql, reply=False)
cb4d662e581feaa3507685c7ce6d1ddc|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill_service.py|57|1|V|python
	    def check_all_bucket(self,coshandler):
	        bucket_list=[self.target_bucket,self.error_bucket,self.backup_bucket]
	        for bucket in bucket_list:
	            self.__check_bucket(coshandler,bucket)
851b2130c34df6f3589df69476fb4d34|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill_service.py|45|1|V|python
	    def __check_bucket(self, coshandler, bucket):
	        if not coshandler.check_bucket(bucket):
	            try:
	                print('create '+bucket+'bucket')
	                coshandler.create_bucket(bucket)
	            except Exception as e:
	                print(e)
f3f0cf8ed59da3ccf5fd220d14529ef3|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill_service.py|17|1|V|python
	import traceback
2ff8eaac31f16df51dfd20cc1461df4c|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill_service.py|15|1|V|python
	import datetime
9caa69c89eb122cc4b722949cfa49cfc|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill_service.py|23|1|V|python
	import script.common.cos_handler as cos_handler
	import script.common.gip_exception as gip_exception
7dca6cfdff4f9ac4441b98672f264af6|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|51|1|V|python
	        self.error_records_filepath='../drill/error_records/'
eee5eea30dbff345db9160f3bfd05f42|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|49|1|V|python
	        self.dic_match = conf_dev['Dic_drill']
3837e5c08a32fd89c82317d1e8c0f43a|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|34|1|V|python
	conf_dev = config_handler.get_config(
	        os.path.join( os.path.dirname(__file__), '../../config/config_dev.json')
	        )
ef21ef92b93a976043c52ca4c723e411|file:///Users/jerry/repo/ibmeww-etl/config/config_dev.json|39|1|V|json
	    "grab.sampleid": [""],
361765d0a0a92c9bf58b6ad6aa98f35e|file:///Users/jerry/repo/ibmeww-etl/config/config_dev.json|40|1|V|json
	    "grab.medium_code": [""],
2fa8c2b4e5131139d8a967cb75c52955|file:///Users/jerry/repo/ibmeww-etl/config/config_dev.json|41|1|V|json
	    "grab.analys_date": [""],
4963bc283c63008554ebd4264f9e6b29|file:///Users/jerry/repo/ibmeww-etl/config/config_dev.json|42|1|V|json
	    "grab.lab_reference": [""],
21f212268b4f8a2714e51b885c15a5be|file:///Users/jerry/repo/ibmeww-etl/config/config_dev.json|43|1|V|json
	    "grab.x": [""],
2930a9591fba5c574ff70549d966cc7a|file:///Users/jerry/repo/ibmeww-etl/config/config_dev.json|44|1|V|json
	    "grab.y": [""],
68202ba4e2a46efd0a0de29b640fb8d2|file:///Users/jerry/repo/ibmeww-etl/config/config_dev.json|46|1|V|json
	    "grab.z": [""],
79f088a921d40941be72ed05d993d76c|file:///Users/jerry/repo/ibmeww-etl/config/config_dev.json|46|1|V|json
	    "grab.au_ppm": [""],
f857ec0ea9cb02a51510981b65928e3e|file:///Users/jerry/repo/ibmeww-etl/config/config_dev.json|47|1|V|json
	    "grab.lithology": [""],
882ec4bd944e74201214ec4a67210383|file:///Users/jerry/repo/ibmeww-etl/config/config_dev.json|48|1|V|json
	    "grab.colour": [""],
ba98dcdf154732152d818ecabbe57bc0|file:///Users/jerry/repo/ibmeww-etl/config/config_dev.json|49|1|V|json
	    "grab.grab_type": [""],
cb8cf6e8fdd5bfc629eb65002ec9db51|file:///Users/jerry/repo/ibmeww-etl/config/config_dev.json|50|1|V|json
	    "grab.grain_size": [""],
2e638a2091893b48c010c5ef5107f56d|file:///Users/jerry/repo/ibmeww-etl/config/config_dev.json|51|1|V|json
	    "grab.foliation": [""],
6721f2bbad3e131c310c11adcdd5a346|file:///Users/jerry/repo/ibmeww-etl/config/config_dev.json|52|1|V|json
	    "grab.weathering": [""],
351efd962d2f659ab7d3ddcc4f1362ab|file:///Users/jerry/repo/ibmeww-etl/config/config_dev.json|53|1|V|json
	    "grab.alt_1": [""],
569ea4d10080e27631bcf62c3e0710d1|file:///Users/jerry/repo/ibmeww-etl/config/config_dev.json|54|1|V|json
	    "grab.alt_1_deg": [""],
d238f1f6a0f776bbf5def1e02a3a15b8|file:///Users/jerry/repo/ibmeww-etl/config/config_dev.json|56|1|V|json
	    "grab.alt_2_deg": [""],
559d143442964e794b7b23e28b27ce34|file:///Users/jerry/repo/ibmeww-etl/config/config_dev.json|57|1|V|json
	    "grab.minz_1": [""],
3eaf84ccf86f101f67abd759f7782baf|file:///Users/jerry/repo/ibmeww-etl/config/config_dev.json|57|1|V|json
	    "grab.minz_1": [""],
	    "grab.minz_1_pct": [""],
b68d31d239844284e03cd6dac5ec6c7a|file:///Users/jerry/repo/ibmeww-etl/config/config_dev.json|60|1|V|json
	    "grab.minz_2_pct": [""],
800fe2ea4abcf706afedf8986ef69ac4|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|476|1|V|python
	        self.pandasData=pd.read_csv(self.filePath,
	                quotechar='"',
	                delimiter=self.fileDelimiter,
	                encoding=self.encoding,
	                low_memory=False,
	                error_bad_lines=False,
	                warn_bad_lines=True,
	                quoting=csv.QUOTE_ALL)
84b86c7e7f09556a18cb0eda409c62ea|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|20|1|V|python
	import numpy as np
	import pandas as pd
39384a652df7681eb614da15a07d3620|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|22|1|V|python
	import csv
f7a73b348f0321716467a27e8b0d3768|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|40|38|v|python
	batchSize=60000, fileDelimiter=','
5474506627906d82381acfb070c1e7f5|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|44|1|V|python
	        self.encoding='cp1252'#old encoding='cp437',current encoding='cp1252'
f70d7dcf7f72edbf0461c7f6465b998d|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|46|1|V|python
	        self.batchSize=batchSize
13c3649a2e933a21d441c6bdbcb21ddc|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|46|1|V|python
	    def detectErrorFormatRecords(self,rawData):
05d5839fd9bb73d59d0e04ad0f255bfe|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|46|1|V|python
	    def detectErrorFormatRecords(self,rawData):
	        i=0
6d63a9b34560500c93d1c86674606017|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|46|1|V|python
	    def detectErrorFormatRecords(self,rawData):
	        i=0
	        j=0
8de0330eb6a7685fa00971b8cfd945bb|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|46|1|V|python
	    def detectErrorFormatRecords(self,rawData):
	        i=0
	        j=0
	        numCol=len(self.pandasData.columns)
ba6e6c77a8a7adbd53fb0904b6caadb1|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|46|1|V|python
	    def detectErrorFormatRecords(self,rawData):
	        i=0
	        j=0
	        numCol=len(self.pandasData.columns)
	        errorRecord=[]
90f63073ef22fc85739d2ba548f57cd8|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|46|1|V|python
	    def detectErrorFormatRecords(self,rawData):
	        i=0
	        j=0
	        numCol=len(self.pandasData.columns)
	        errorRecord=[]
	        errorRecord_colNum = []
c12ddcfffbd59479fbd0348635edd4e2|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|46|1|V|python
	    def detectErrorFormatRecords(self,rawData):
	        i=0
	        j=0
	        numCol=len(self.pandasData.columns)
	        errorRecord=[]
	        errorRecord_colNum = []
	        uniqueIndex=[]
ca24438087799cb33e1585eec1676c9a|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|46|1|V|python
	    def detectErrorFormatRecords(self,rawData):
	        i=0
	        j=0
	        numCol=len(self.pandasData.columns)
	        errorRecord=[]
	        errorRecord_colNum = []
	        uniqueIndex=[]
	        print(self.uniqueKey)
1cc46d029442122b2ff57b35ed48da4f|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|46|1|V|python
	    def detectErrorFormatRecords(self,rawData):
	        i=0
	        j=0
	        numCol=len(self.pandasData.columns)
	        errorRecord=[]
	        errorRecord_colNum = []
	        uniqueIndex=[]
	        print(self.uniqueKey)
	        print(self.pandasData.columns)
d192800f3cb049fcfbddc729d111e5fc|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|46|1|V|python
	    def detectErrorFormatRecords(self,rawData):
	        i=0
	        j=0
	        numCol=len(self.pandasData.columns)
	        errorRecord=[]
	        errorRecord_colNum = []
	        uniqueIndex=[]
	        print(self.uniqueKey)
	        print(self.pandasData.columns)
	        for key in self.uniqueKey:
ea163e1b8b546d29202d3fa03b488f4a|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|46|1|V|python
	    def detectErrorFormatRecords(self,rawData):
	        i=0
	        j=0
	        numCol=len(self.pandasData.columns)
	        errorRecord=[]
	        errorRecord_colNum = []
	        uniqueIndex=[]
	        print(self.uniqueKey)
	        print(self.pandasData.columns)
	        for key in self.uniqueKey:
	            try:
e8294e7900bc2585bb7d5ba1d6b6518b|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|46|1|V|python
	    def detectErrorFormatRecords(self,rawData):
	        i=0
	        j=0
	        numCol=len(self.pandasData.columns)
	        errorRecord=[]
	        errorRecord_colNum = []
	        uniqueIndex=[]
	        print(self.uniqueKey)
	        print(self.pandasData.columns)
	        for key in self.uniqueKey:
	            try:
	                uniqueIndex.append(list(self.pandasData.columns).index(key))
cf53c8a197d95d2b0e359c6074f96de6|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|46|1|V|python
	    def detectErrorFormatRecords(self,rawData):
	        i=0
	        j=0
	        numCol=len(self.pandasData.columns)
	        errorRecord=[]
	        errorRecord_colNum = []
	        uniqueIndex=[]
	        print(self.uniqueKey)
	        print(self.pandasData.columns)
	        for key in self.uniqueKey:
	            try:
	                uniqueIndex.append(list(self.pandasData.columns).index(key))
	            except :
874ce2ea31e125f699a6390a68a2ec5f|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|46|1|V|python
	    def detectErrorFormatRecords(self,rawData):
	        i=0
	        j=0
	        numCol=len(self.pandasData.columns)
	        errorRecord=[]
	        errorRecord_colNum = []
	        uniqueIndex=[]
	        print(self.uniqueKey)
	        print(self.pandasData.columns)
	        for key in self.uniqueKey:
	            try:
	                uniqueIndex.append(list(self.pandasData.columns).index(key))
	            except :
	                Exception
8e64b5b4ec2bc2c55f112e9f92b1718c|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|46|1|V|python
	    def detectErrorFormatRecords(self,rawData):
	        i=0
	        j=0
	        numCol=len(self.pandasData.columns)
	        errorRecord=[]
	        errorRecord_colNum = []
	        uniqueIndex=[]
	        print(self.uniqueKey)
	        print(self.pandasData.columns)
	        for key in self.uniqueKey:
	            try:
	                uniqueIndex.append(list(self.pandasData.columns).index(key))
	            except :
	                Exception
	        pfData=self.pandasData.values
3e558d28a099eca7bf127317b6b9dac2|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|46|1|V|python
	    def detectErrorFormatRecords(self,rawData):
	        i=0
	        j=0
	        numCol=len(self.pandasData.columns)
	        errorRecord=[]
	        errorRecord_colNum = []
	        uniqueIndex=[]
	        print(self.uniqueKey)
	        print(self.pandasData.columns)
	        for key in self.uniqueKey:
	            try:
	                uniqueIndex.append(list(self.pandasData.columns).index(key))
	            except :
	                Exception
	        pfData=self.pandasData.values
	        k=0
47a5929d2609ca0cf817c51c1afda60c|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|46|1|V|python
	    def detectErrorFormatRecords(self,rawData):
	        i=0
	        j=0
	        numCol=len(self.pandasData.columns)
	        errorRecord=[]
	        errorRecord_colNum = []
	        uniqueIndex=[]
	        print(self.uniqueKey)
	        print(self.pandasData.columns)
	        for key in self.uniqueKey:
	            try:
	                uniqueIndex.append(list(self.pandasData.columns).index(key))
	            except :
	                Exception
	        pfData=self.pandasData.values
	        k=0
	        if len(pfData)==len(rawData):
f4f1f07924a7fbe2fbcd07036d55e2f1|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|46|1|V|python
	    def detectErrorFormatRecords(self,rawData):
	        i=0
	        j=0
	        numCol=len(self.pandasData.columns)
	        errorRecord=[]
	        errorRecord_colNum = []
	        uniqueIndex=[]
	        print(self.uniqueKey)
	        print(self.pandasData.columns)
	        for key in self.uniqueKey:
	            try:
	                uniqueIndex.append(list(self.pandasData.columns).index(key))
	            except :
	                Exception
	        pfData=self.pandasData.values
	        k=0
	        if len(pfData)==len(rawData):
	            return []
6bb6a463cada53c2abcecb72c79ed128|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|46|1|V|python
	    def detectErrorFormatRecords(self,rawData):
	        i=0
	        j=0
	        numCol=len(self.pandasData.columns)
	        errorRecord=[]
	        errorRecord_colNum = []
	        uniqueIndex=[]
	        print(self.uniqueKey)
	        print(self.pandasData.columns)
	        for key in self.uniqueKey:
	            try:
	                uniqueIndex.append(list(self.pandasData.columns).index(key))
	            except :
	                Exception
	        pfData=self.pandasData.values
	        k=0
	        if len(pfData)==len(rawData):
	            return []
	        while i<len(pfData) and j<len(rawData):
4794cd043109cc1616bd6eb36baf9981|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|46|1|V|python
	    def detectErrorFormatRecords(self,rawData):
	        i=0
	        j=0
	        numCol=len(self.pandasData.columns)
	        errorRecord=[]
	        errorRecord_colNum = []
	        uniqueIndex=[]
	        print(self.uniqueKey)
	        print(self.pandasData.columns)
	        for key in self.uniqueKey:
	            try:
	                uniqueIndex.append(list(self.pandasData.columns).index(key))
	            except :
	                Exception
	        pfData=self.pandasData.values
	        k=0
	        if len(pfData)==len(rawData):
	            return []
	        while i<len(pfData) and j<len(rawData):
	            if pfData[i][uniqueIndex[0]]==rawData[j][uniqueIndex[0]] and abs(pfData[i][uniqueIndex[1]]-float(rawData[j][uniqueIndex[1]]))<0.0001:
9a67a6690481b757f67b2389d47b8a77|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|46|1|V|python
	    def detectErrorFormatRecords(self,rawData):
	        i=0
	        j=0
	        numCol=len(self.pandasData.columns)
	        errorRecord=[]
	        errorRecord_colNum = []
	        uniqueIndex=[]
	        print(self.uniqueKey)
	        print(self.pandasData.columns)
	        for key in self.uniqueKey:
	            try:
	                uniqueIndex.append(list(self.pandasData.columns).index(key))
	            except :
	                Exception
	        pfData=self.pandasData.values
	        k=0
	        if len(pfData)==len(rawData):
	            return []
	        while i<len(pfData) and j<len(rawData):
	            if pfData[i][uniqueIndex[0]]==rawData[j][uniqueIndex[0]] and abs(pfData[i][uniqueIndex[1]]-float(rawData[j][uniqueIndex[1]]))<0.0001:
	                if len(pfData[i])!=numCol:
37e73cf38925323b1df5ae3a57cafe9d|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|46|1|V|python
	    def detectErrorFormatRecords(self,rawData):
	        i=0
	        j=0
	        numCol=len(self.pandasData.columns)
	        errorRecord=[]
	        errorRecord_colNum = []
	        uniqueIndex=[]
	        print(self.uniqueKey)
	        print(self.pandasData.columns)
	        for key in self.uniqueKey:
	            try:
	                uniqueIndex.append(list(self.pandasData.columns).index(key))
	            except :
	                Exception
	        pfData=self.pandasData.values
	        k=0
	        if len(pfData)==len(rawData):
	            return []
	        while i<len(pfData) and j<len(rawData):
	            if pfData[i][uniqueIndex[0]]==rawData[j][uniqueIndex[0]] and abs(pfData[i][uniqueIndex[1]]-float(rawData[j][uniqueIndex[1]]))<0.0001:
	                if len(pfData[i])!=numCol:
	                    errorRecord_colNum.append(rawData[j])
35708d978389d93f97099191c6aa7405|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|46|1|V|python
	    def detectErrorFormatRecords(self,rawData):
	        i=0
	        j=0
	        numCol=len(self.pandasData.columns)
	        errorRecord=[]
	        errorRecord_colNum = []
	        uniqueIndex=[]
	        print(self.uniqueKey)
	        print(self.pandasData.columns)
	        for key in self.uniqueKey:
	            try:
	                uniqueIndex.append(list(self.pandasData.columns).index(key))
	            except :
	                Exception
	        pfData=self.pandasData.values
	        k=0
	        if len(pfData)==len(rawData):
	            return []
	        while i<len(pfData) and j<len(rawData):
	            if pfData[i][uniqueIndex[0]]==rawData[j][uniqueIndex[0]] and abs(pfData[i][uniqueIndex[1]]-float(rawData[j][uniqueIndex[1]]))<0.0001:
	                if len(pfData[i])!=numCol:
	                    errorRecord_colNum.append(rawData[j])
	                i+=1
f9b79ebd5734659e590bfde7841ddec8|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|46|1|V|python
	    def detectErrorFormatRecords(self,rawData):
	        i=0
	        j=0
	        numCol=len(self.pandasData.columns)
	        errorRecord=[]
	        errorRecord_colNum = []
	        uniqueIndex=[]
	        print(self.uniqueKey)
	        print(self.pandasData.columns)
	        for key in self.uniqueKey:
	            try:
	                uniqueIndex.append(list(self.pandasData.columns).index(key))
	            except :
	                Exception
	        pfData=self.pandasData.values
	        k=0
	        if len(pfData)==len(rawData):
	            return []
	        while i<len(pfData) and j<len(rawData):
	            if pfData[i][uniqueIndex[0]]==rawData[j][uniqueIndex[0]] and abs(pfData[i][uniqueIndex[1]]-float(rawData[j][uniqueIndex[1]]))<0.0001:
	                if len(pfData[i])!=numCol:
	                    errorRecord_colNum.append(rawData[j])
	                i+=1
	                j+=1
6c2786f75e9beb45799e2e5c5c916334|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|46|1|V|python
	    def detectErrorFormatRecords(self,rawData):
	        i=0
	        j=0
	        numCol=len(self.pandasData.columns)
	        errorRecord=[]
	        errorRecord_colNum = []
	        uniqueIndex=[]
	        print(self.uniqueKey)
	        print(self.pandasData.columns)
	        for key in self.uniqueKey:
	            try:
	                uniqueIndex.append(list(self.pandasData.columns).index(key))
	            except :
	                Exception
	        pfData=self.pandasData.values
	        k=0
	        if len(pfData)==len(rawData):
	            return []
	        while i<len(pfData) and j<len(rawData):
	            if pfData[i][uniqueIndex[0]]==rawData[j][uniqueIndex[0]] and abs(pfData[i][uniqueIndex[1]]-float(rawData[j][uniqueIndex[1]]))<0.0001:
	                if len(pfData[i])!=numCol:
	                    errorRecord_colNum.append(rawData[j])
	                i+=1
	                j+=1
	            else:
7c10fd0c1d2e71185dbd679a2d3ae14a|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|46|1|V|python
	    def detectErrorFormatRecords(self,rawData):
	        i=0
	        j=0
	        numCol=len(self.pandasData.columns)
	        errorRecord=[]
	        errorRecord_colNum = []
	        uniqueIndex=[]
	        print(self.uniqueKey)
	        print(self.pandasData.columns)
	        for key in self.uniqueKey:
	            try:
	                uniqueIndex.append(list(self.pandasData.columns).index(key))
	            except :
	                Exception
	        pfData=self.pandasData.values
	        k=0
	        if len(pfData)==len(rawData):
	            return []
	        while i<len(pfData) and j<len(rawData):
	            if pfData[i][uniqueIndex[0]]==rawData[j][uniqueIndex[0]] and abs(pfData[i][uniqueIndex[1]]-float(rawData[j][uniqueIndex[1]]))<0.0001:
	                if len(pfData[i])!=numCol:
	                    errorRecord_colNum.append(rawData[j])
	                i+=1
	                j+=1
	            else:
	                errorRecord.append(rawData[j])
697e5e9c85ebca750854c59a5d604cf7|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|46|1|V|python
	    def detectErrorFormatRecords(self,rawData):
	        i=0
	        j=0
	        numCol=len(self.pandasData.columns)
	        errorRecord=[]
	        errorRecord_colNum = []
	        uniqueIndex=[]
	        print(self.uniqueKey)
	        print(self.pandasData.columns)
	        for key in self.uniqueKey:
	            try:
	                uniqueIndex.append(list(self.pandasData.columns).index(key))
	            except :
	                Exception
	        pfData=self.pandasData.values
	        k=0
	        if len(pfData)==len(rawData):
	            return []
	        while i<len(pfData) and j<len(rawData):
	            if pfData[i][uniqueIndex[0]]==rawData[j][uniqueIndex[0]] and abs(pfData[i][uniqueIndex[1]]-float(rawData[j][uniqueIndex[1]]))<0.0001:
	                if len(pfData[i])!=numCol:
	                    errorRecord_colNum.append(rawData[j])
	                i+=1
	                j+=1
	            else:
	                errorRecord.append(rawData[j])
	                j+=1
7170d4fcc5f7bd86ac65b069c80c5946|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|46|1|V|python
	    def detectErrorFormatRecords(self,rawData):
	        i=0
	        j=0
	        numCol=len(self.pandasData.columns)
	        errorRecord=[]
	        errorRecord_colNum = []
	        uniqueIndex=[]
	        print(self.uniqueKey)
	        print(self.pandasData.columns)
	        for key in self.uniqueKey:
	            try:
	                uniqueIndex.append(list(self.pandasData.columns).index(key))
	            except :
	                Exception
	        pfData=self.pandasData.values
	        k=0
	        if len(pfData)==len(rawData):
	            return []
	        while i<len(pfData) and j<len(rawData):
	            if pfData[i][uniqueIndex[0]]==rawData[j][uniqueIndex[0]] and abs(pfData[i][uniqueIndex[1]]-float(rawData[j][uniqueIndex[1]]))<0.0001:
	                if len(pfData[i])!=numCol:
	                    errorRecord_colNum.append(rawData[j])
	                i+=1
	                j+=1
	            else:
	                errorRecord.append(rawData[j])
	                j+=1
	                k+=1
8d7487c3dfd9ba679310475d2c214f6b|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|46|1|V|python
	    def detectErrorFormatRecords(self,rawData):
	        i=0
	        j=0
	        numCol=len(self.pandasData.columns)
	        errorRecord=[]
	        errorRecord_colNum = []
	        uniqueIndex=[]
	        print(self.uniqueKey)
	        print(self.pandasData.columns)
	        for key in self.uniqueKey:
	            try:
	                uniqueIndex.append(list(self.pandasData.columns).index(key))
	            except :
	                Exception
	        pfData=self.pandasData.values
	        k=0
	        if len(pfData)==len(rawData):
	            return []
	        while i<len(pfData) and j<len(rawData):
	            if pfData[i][uniqueIndex[0]]==rawData[j][uniqueIndex[0]] and abs(pfData[i][uniqueIndex[1]]-float(rawData[j][uniqueIndex[1]]))<0.0001:
	                if len(pfData[i])!=numCol:
	                    errorRecord_colNum.append(rawData[j])
	                i+=1
	                j+=1
	            else:
	                errorRecord.append(rawData[j])
	                j+=1
	                k+=1
	        try:
54b99ac99420578a29abd6afdbc2cb00|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|46|1|V|python
	    def detectErrorFormatRecords(self,rawData):
	        i=0
	        j=0
	        numCol=len(self.pandasData.columns)
	        errorRecord=[]
	        errorRecord_colNum = []
	        uniqueIndex=[]
	        print(self.uniqueKey)
	        print(self.pandasData.columns)
	        for key in self.uniqueKey:
	            try:
	                uniqueIndex.append(list(self.pandasData.columns).index(key))
	            except :
	                Exception
	        pfData=self.pandasData.values
	        k=0
	        if len(pfData)==len(rawData):
	            return []
	        while i<len(pfData) and j<len(rawData):
	            if pfData[i][uniqueIndex[0]]==rawData[j][uniqueIndex[0]] and abs(pfData[i][uniqueIndex[1]]-float(rawData[j][uniqueIndex[1]]))<0.0001:
	                if len(pfData[i])!=numCol:
	                    errorRecord_colNum.append(rawData[j])
	                i+=1
	                j+=1
	            else:
	                errorRecord.append(rawData[j])
	                j+=1
	                k+=1
	        try:
	            errorRecord=pd.DataFrame(errorRecord)
c5151e12efbcdf7c6b567737f60d2a2e|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|46|1|V|python
	    def detectErrorFormatRecords(self,rawData):
	        i=0
	        j=0
	        numCol=len(self.pandasData.columns)
	        errorRecord=[]
	        errorRecord_colNum = []
	        uniqueIndex=[]
	        print(self.uniqueKey)
	        print(self.pandasData.columns)
	        for key in self.uniqueKey:
	            try:
	                uniqueIndex.append(list(self.pandasData.columns).index(key))
	            except :
	                Exception
	        pfData=self.pandasData.values
	        k=0
	        if len(pfData)==len(rawData):
	            return []
	        while i<len(pfData) and j<len(rawData):
	            if pfData[i][uniqueIndex[0]]==rawData[j][uniqueIndex[0]] and abs(pfData[i][uniqueIndex[1]]-float(rawData[j][uniqueIndex[1]]))<0.0001:
	                if len(pfData[i])!=numCol:
	                    errorRecord_colNum.append(rawData[j])
	                i+=1
	                j+=1
	            else:
	                errorRecord.append(rawData[j])
	                j+=1
	                k+=1
	        try:
	            errorRecord=pd.DataFrame(errorRecord)
	            errorRecord_colNum=pd.DataFrame(errorRecord_colNum)
a642add4cdae841cf9176fe77f0dbb65|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|46|1|V|python
	    def detectErrorFormatRecords(self,rawData):
	        i=0
	        j=0
	        numCol=len(self.pandasData.columns)
	        errorRecord=[]
	        errorRecord_colNum = []
	        uniqueIndex=[]
	        print(self.uniqueKey)
	        print(self.pandasData.columns)
	        for key in self.uniqueKey:
	            try:
	                uniqueIndex.append(list(self.pandasData.columns).index(key))
	            except :
	                Exception
	        pfData=self.pandasData.values
	        k=0
	        if len(pfData)==len(rawData):
	            return []
	        while i<len(pfData) and j<len(rawData):
	            if pfData[i][uniqueIndex[0]]==rawData[j][uniqueIndex[0]] and abs(pfData[i][uniqueIndex[1]]-float(rawData[j][uniqueIndex[1]]))<0.0001:
	                if len(pfData[i])!=numCol:
	                    errorRecord_colNum.append(rawData[j])
	                i+=1
	                j+=1
	            else:
	                errorRecord.append(rawData[j])
	                j+=1
	                k+=1
	        try:
	            errorRecord=pd.DataFrame(errorRecord)
	            errorRecord_colNum=pd.DataFrame(errorRecord_colNum)
	        except:
9bba312dafd6b94dfad208127115b7e3|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|46|1|V|python
	    def detectErrorFormatRecords(self,rawData):
	        i=0
	        j=0
	        numCol=len(self.pandasData.columns)
	        errorRecord=[]
	        errorRecord_colNum = []
	        uniqueIndex=[]
	        print(self.uniqueKey)
	        print(self.pandasData.columns)
	        for key in self.uniqueKey:
	            try:
	                uniqueIndex.append(list(self.pandasData.columns).index(key))
	            except :
	                Exception
	        pfData=self.pandasData.values
	        k=0
	        if len(pfData)==len(rawData):
	            return []
	        while i<len(pfData) and j<len(rawData):
	            if pfData[i][uniqueIndex[0]]==rawData[j][uniqueIndex[0]] and abs(pfData[i][uniqueIndex[1]]-float(rawData[j][uniqueIndex[1]]))<0.0001:
	                if len(pfData[i])!=numCol:
	                    errorRecord_colNum.append(rawData[j])
	                i+=1
	                j+=1
	            else:
	                errorRecord.append(rawData[j])
	                j+=1
	                k+=1
	        try:
	            errorRecord=pd.DataFrame(errorRecord)
	            errorRecord_colNum=pd.DataFrame(errorRecord_colNum)
	        except:
	            Exception
41b0a64ba65b85cd990bdc5b28f7e2a2|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|46|1|V|python
	    def detectErrorFormatRecords(self,rawData):
	        i=0
	        j=0
	        numCol=len(self.pandasData.columns)
	        errorRecord=[]
	        errorRecord_colNum = []
	        uniqueIndex=[]
	        print(self.uniqueKey)
	        print(self.pandasData.columns)
	        for key in self.uniqueKey:
	            try:
	                uniqueIndex.append(list(self.pandasData.columns).index(key))
	            except :
	                Exception
	        pfData=self.pandasData.values
	        k=0
	        if len(pfData)==len(rawData):
	            return []
	        while i<len(pfData) and j<len(rawData):
	            if pfData[i][uniqueIndex[0]]==rawData[j][uniqueIndex[0]] and abs(pfData[i][uniqueIndex[1]]-float(rawData[j][uniqueIndex[1]]))<0.0001:
	                if len(pfData[i])!=numCol:
	                    errorRecord_colNum.append(rawData[j])
	                i+=1
	                j+=1
	            else:
	                errorRecord.append(rawData[j])
	                j+=1
	                k+=1
	        try:
	            errorRecord=pd.DataFrame(errorRecord)
	            errorRecord_colNum=pd.DataFrame(errorRecord_colNum)
	        except:
	            Exception
	        self.logErrorRecord(errorRecord, self.fileName + '_error_format_records.csv')
2808201b232a5c55e8724a14701aa828|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|46|1|V|python
	    def detectErrorFormatRecords(self,rawData):
	        i=0
	        j=0
	        numCol=len(self.pandasData.columns)
	        errorRecord=[]
	        errorRecord_colNum = []
	        uniqueIndex=[]
	        print(self.uniqueKey)
	        print(self.pandasData.columns)
	        for key in self.uniqueKey:
	            try:
	                uniqueIndex.append(list(self.pandasData.columns).index(key))
	            except :
	                Exception
	        pfData=self.pandasData.values
	        k=0
	        if len(pfData)==len(rawData):
	            return []
	        while i<len(pfData) and j<len(rawData):
	            if pfData[i][uniqueIndex[0]]==rawData[j][uniqueIndex[0]] and abs(pfData[i][uniqueIndex[1]]-float(rawData[j][uniqueIndex[1]]))<0.0001:
	                if len(pfData[i])!=numCol:
	                    errorRecord_colNum.append(rawData[j])
	                i+=1
	                j+=1
	            else:
	                errorRecord.append(rawData[j])
	                j+=1
	                k+=1
	        try:
	            errorRecord=pd.DataFrame(errorRecord)
	            errorRecord_colNum=pd.DataFrame(errorRecord_colNum)
	        except:
	            Exception
	        self.logErrorRecord(errorRecord, self.fileName + '_error_format_records.csv')
	        self.logErrorRecord(errorRecord_colNum, self.fileName + '_error_column_number_records_.csv')
0abd6cf38f905d51cc60498007dbf7e5|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|280|1|V|python
	    def logErrorRecord(self,errorRecord,errorFileName):
	        errorFilePath=self.error_records_filepath+errorFileName
	        if len(errorRecord)==0:
	            return
	        try:
	            errorRecord.to_csv(errorFilePath)
	        except Exception as e:
	            file=open(errorFilePath,'w',encoding=self.encoding)
	            file.write(str(errorRecord).replace('[','')).replace(']','\n')
	            file.close()
63cc33c7a2ec57a37394ec59d32f045d|file:///Users/jerry/repo/ibmeww-etl/script/surface/surface.py|31|14|v|python
	error_records_filepath
b4a648bcb8ec1a8227ec4b90fe5b0072|file:///Users/jerry/repo/ibmeww-etl/script/common/base_loader.py|38|16|v|python
	error_records_filepath 
2e1781c3ae3c75ac4ee28ef3ecc60a35|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|280|1|V|python
	    def detectErrorFormatRecords(self,rawData):
	        i=0
	        j=0
	        numCol=len(self.pandasData.columns)
	        errorRecord=[]
	        errorRecord_colNum = []
	        uniqueIndex=[]
	        print(self.uniqueKey)
	        print(self.pandasData.columns)
	        for key in self.uniqueKey:
	            try:
	                uniqueIndex.append(list(self.pandasData.columns).index(key))
	            except :
	                Exception
	        pfData=self.pandasData.values
	        k=0
	        if len(pfData)==len(rawData):
	            return []
	        while i<len(pfData) and j<len(rawData):
	            if pfData[i][uniqueIndex[0]]==rawData[j][uniqueIndex[0]] and abs(pfData[i][uniqueIndex[1]]-float(rawData[j][uniqueIndex[1]]))<0.0001:
	                if len(pfData[i])!=numCol:
	                    errorRecord_colNum.append(rawData[j])
	                i+=1
	                j+=1
	            else:
	                errorRecord.append(rawData[j])
	                j+=1
	                k+=1
	        try:
	            errorRecord=pd.DataFrame(errorRecord)
	            errorRecord_colNum=pd.DataFrame(errorRecord_colNum)
	        except:
	            Exception
	        self.logErrorRecord(errorRecord, self.fileName + '_error_format_records.csv')
	        self.logErrorRecord(errorRecord_colNum, self.fileName + '_error_column_number_records_.csv')
	        return errorRecord
c7c39dc811a5f94c833b0ae2f560534f|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|487|1|V|python
	        lines=[]
	        with open(self.filePath,'r',encoding=self.encoding) as file:
	            #  lines = list(csv.reader(file, quotechar='"', delimiter=',',
	                     #  quoting=csv.QUOTE_ALL, skipinitialspace=True))
	            for line in file:
	                #  lines.append(line.split(self.fileDelimiter))
	                lines.append(line)
	        lines = list(csv.reader(lines, quotechar='"', delimiter=',',
	                 quoting=csv.QUOTE_ALL, skipinitialspace=True))
	        rawData=lines[1:len(lines)]
083b4acb3504125d375f312d93d44c4d|file:///Users/jerry/repo/ibmeww-etl/script/surface/surface.py|32|14|v|python
	dic_match 
3ef40b8c035a38ae2c265e1be549ccd0|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|300|26|v|python
	dic_notnull
2406d0e059b2bfdb1d676ddca89abee0|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|50|38|v|python
	notNullColumns
25dfd7108cd6863d175e80bc0581f9e9|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|50|38|V|python
	    def filterNullData(self,tableData,tableColumns,table,order):
038a60c3753b4a00998ccc6ebc16e913|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|50|38|V|python
	    def filterNullData(self,tableData,tableColumns,table,order):
	        dic_notnull=self.dic_notnull
e9acd30f68d927139d3d823af37d0164|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|50|38|V|python
	    def filterNullData(self,tableData,tableColumns,table,order):
	        dic_notnull=self.dic_notnull
	        null_data_list=[]
4b6350ad29de91bc319b8ce37e006062|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|50|38|V|python
	    def filterNullData(self,tableData,tableColumns,table,order):
	        dic_notnull=self.dic_notnull
	        null_data_list=[]
	        for col in tableColumns:
c706d12c728131c1f5e9d79d3a670faa|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|50|38|V|python
	    def filterNullData(self,tableData,tableColumns,table,order):
	        dic_notnull=self.dic_notnull
	        null_data_list=[]
	        for col in tableColumns:
	            try:
548c8bc01610c67991c89943341921b4|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|50|38|V|python
	    def filterNullData(self,tableData,tableColumns,table,order):
	        dic_notnull=self.dic_notnull
	        null_data_list=[]
	        for col in tableColumns:
	            try:
	                if col in dic_notnull[table.replace('_tmp', '')]:
e1a4e42c1a16e4fa06e10f03eb9c8f7d|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|50|38|V|python
	    def filterNullData(self,tableData,tableColumns,table,order):
	        dic_notnull=self.dic_notnull
	        null_data_list=[]
	        for col in tableColumns:
	            try:
	                if col in dic_notnull[table.replace('_tmp', '')]:
	                    notNullColumn=self.matchByConfig(table,col,order)
1728ee9a8109c793c971842574454bba|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|50|38|V|python
	    def filterNullData(self,tableData,tableColumns,table,order):
	        dic_notnull=self.dic_notnull
	        null_data_list=[]
	        for col in tableColumns:
	            try:
	                if col in dic_notnull[table.replace('_tmp', '')]:
	                    notNullColumn=self.matchByConfig(table,col,order)
	                    null_data = tableData[tableData[notNullColumn].isnull() ]
28616f49a38223fef96fa85cb7d9113c|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|50|38|V|python
	    def filterNullData(self,tableData,tableColumns,table,order):
	        dic_notnull=self.dic_notnull
	        null_data_list=[]
	        for col in tableColumns:
	            try:
	                if col in dic_notnull[table.replace('_tmp', '')]:
	                    notNullColumn=self.matchByConfig(table,col,order)
	                    null_data = tableData[tableData[notNullColumn].isnull() ]
	                    try:
c76165004bedb28d9de2c40b2b3ecac2|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|50|38|V|python
	    def filterNullData(self,tableData,tableColumns,table,order):
	        dic_notnull=self.dic_notnull
	        null_data_list=[]
	        for col in tableColumns:
	            try:
	                if col in dic_notnull[table.replace('_tmp', '')]:
	                    notNullColumn=self.matchByConfig(table,col,order)
	                    null_data = tableData[tableData[notNullColumn].isnull() ]
	                    try:
	                        null_data=null_data.concat(tableData[tableData[notNullColumn] == '-'])
9786969c58de22e083b6a2f37af8355d|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|50|38|V|python
	    def filterNullData(self,tableData,tableColumns,table,order):
	        dic_notnull=self.dic_notnull
	        null_data_list=[]
	        for col in tableColumns:
	            try:
	                if col in dic_notnull[table.replace('_tmp', '')]:
	                    notNullColumn=self.matchByConfig(table,col,order)
	                    null_data = tableData[tableData[notNullColumn].isnull() ]
	                    try:
	                        null_data=null_data.concat(tableData[tableData[notNullColumn] == '-'])
	                    except:
1aa3bd358136001b8e3d6d79ea475e16|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|50|38|V|python
	    def filterNullData(self,tableData,tableColumns,table,order):
	        dic_notnull=self.dic_notnull
	        null_data_list=[]
	        for col in tableColumns:
	            try:
	                if col in dic_notnull[table.replace('_tmp', '')]:
	                    notNullColumn=self.matchByConfig(table,col,order)
	                    null_data = tableData[tableData[notNullColumn].isnull() ]
	                    try:
	                        null_data=null_data.concat(tableData[tableData[notNullColumn] == '-'])
	                    except:
	                        Exception
52118ccb0d64e6c74b12c6c48e4ca04a|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|50|38|V|python
	    def filterNullData(self,tableData,tableColumns,table,order):
	        dic_notnull=self.dic_notnull
	        null_data_list=[]
	        for col in tableColumns:
	            try:
	                if col in dic_notnull[table.replace('_tmp', '')]:
	                    notNullColumn=self.matchByConfig(table,col,order)
	                    null_data = tableData[tableData[notNullColumn].isnull() ]
	                    try:
	                        null_data=null_data.concat(tableData[tableData[notNullColumn] == '-'])
	                    except:
	                        Exception
	                    if len(null_data)!=0:
d30ced2837cc66794d39a552bb8aae06|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|50|38|V|python
	    def filterNullData(self,tableData,tableColumns,table,order):
	        dic_notnull=self.dic_notnull
	        null_data_list=[]
	        for col in tableColumns:
	            try:
	                if col in dic_notnull[table.replace('_tmp', '')]:
	                    notNullColumn=self.matchByConfig(table,col,order)
	                    null_data = tableData[tableData[notNullColumn].isnull() ]
	                    try:
	                        null_data=null_data.concat(tableData[tableData[notNullColumn] == '-'])
	                    except:
	                        Exception
	                    if len(null_data)!=0:
	                        null_data['comments']=np.array([col+' is null'] *len(null_data))
ff455618adcdd8b291f2276ccae44dda|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|50|38|V|python
	    def filterNullData(self,tableData,tableColumns,table,order):
	        dic_notnull=self.dic_notnull
	        null_data_list=[]
	        for col in tableColumns:
	            try:
	                if col in dic_notnull[table.replace('_tmp', '')]:
	                    notNullColumn=self.matchByConfig(table,col,order)
	                    null_data = tableData[tableData[notNullColumn].isnull() ]
	                    try:
	                        null_data=null_data.concat(tableData[tableData[notNullColumn] == '-'])
	                    except:
	                        Exception
	                    if len(null_data)!=0:
	                        null_data['comments']=np.array([col+' is null'] *len(null_data))
	                        null_data_list.append(null_data)
6a73ea069057e23f43abc6c6eacbae08|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|50|38|V|python
	    def filterNullData(self,tableData,tableColumns,table,order):
	        dic_notnull=self.dic_notnull
	        null_data_list=[]
	        for col in tableColumns:
	            try:
	                if col in dic_notnull[table.replace('_tmp', '')]:
	                    notNullColumn=self.matchByConfig(table,col,order)
	                    null_data = tableData[tableData[notNullColumn].isnull() ]
	                    try:
	                        null_data=null_data.concat(tableData[tableData[notNullColumn] == '-'])
	                    except:
	                        Exception
	                    if len(null_data)!=0:
	                        null_data['comments']=np.array([col+' is null'] *len(null_data))
	                        null_data_list.append(null_data)
	                        tableData = tableData[~tableData[notNullColumn].isnull()]
859f51091fe1792c6dc079cf1f617506|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|50|38|V|python
	    def filterNullData(self,tableData,tableColumns,table,order):
	        dic_notnull=self.dic_notnull
	        null_data_list=[]
	        for col in tableColumns:
	            try:
	                if col in dic_notnull[table.replace('_tmp', '')]:
	                    notNullColumn=self.matchByConfig(table,col,order)
	                    null_data = tableData[tableData[notNullColumn].isnull() ]
	                    try:
	                        null_data=null_data.concat(tableData[tableData[notNullColumn] == '-'])
	                    except:
	                        Exception
	                    if len(null_data)!=0:
	                        null_data['comments']=np.array([col+' is null'] *len(null_data))
	                        null_data_list.append(null_data)
	                        tableData = tableData[~tableData[notNullColumn].isnull()]
	                        try:
fea238ecf2cebb08e522abb6ac9af89e|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|50|38|V|python
	    def filterNullData(self,tableData,tableColumns,table,order):
	        dic_notnull=self.dic_notnull
	        null_data_list=[]
	        for col in tableColumns:
	            try:
	                if col in dic_notnull[table.replace('_tmp', '')]:
	                    notNullColumn=self.matchByConfig(table,col,order)
	                    null_data = tableData[tableData[notNullColumn].isnull() ]
	                    try:
	                        null_data=null_data.concat(tableData[tableData[notNullColumn] == '-'])
	                    except:
	                        Exception
	                    if len(null_data)!=0:
	                        null_data['comments']=np.array([col+' is null'] *len(null_data))
	                        null_data_list.append(null_data)
	                        tableData = tableData[~tableData[notNullColumn].isnull()]
	                        try:
	                            tableData = tableData[tableData[notNullColumn]!='-']
08f0d0dc67960658229e1350b5c0bdb7|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|50|38|V|python
	    def filterNullData(self,tableData,tableColumns,table,order):
	        dic_notnull=self.dic_notnull
	        null_data_list=[]
	        for col in tableColumns:
	            try:
	                if col in dic_notnull[table.replace('_tmp', '')]:
	                    notNullColumn=self.matchByConfig(table,col,order)
	                    null_data = tableData[tableData[notNullColumn].isnull() ]
	                    try:
	                        null_data=null_data.concat(tableData[tableData[notNullColumn] == '-'])
	                    except:
	                        Exception
	                    if len(null_data)!=0:
	                        null_data['comments']=np.array([col+' is null'] *len(null_data))
	                        null_data_list.append(null_data)
	                        tableData = tableData[~tableData[notNullColumn].isnull()]
	                        try:
	                            tableData = tableData[tableData[notNullColumn]!='-']
	                        except:
6fc2459c28127be15abd8d432c6a5073|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|50|38|V|python
	    def filterNullData(self,tableData,tableColumns,table,order):
	        dic_notnull=self.dic_notnull
	        null_data_list=[]
	        for col in tableColumns:
	            try:
	                if col in dic_notnull[table.replace('_tmp', '')]:
	                    notNullColumn=self.matchByConfig(table,col,order)
	                    null_data = tableData[tableData[notNullColumn].isnull() ]
	                    try:
	                        null_data=null_data.concat(tableData[tableData[notNullColumn] == '-'])
	                    except:
	                        Exception
	                    if len(null_data)!=0:
	                        null_data['comments']=np.array([col+' is null'] *len(null_data))
	                        null_data_list.append(null_data)
	                        tableData = tableData[~tableData[notNullColumn].isnull()]
	                        try:
	                            tableData = tableData[tableData[notNullColumn]!='-']
	                        except:
	                            Exception
0b3d3deb8b1ca50729201179dc32b765|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|50|38|V|python
	    def filterNullData(self,tableData,tableColumns,table,order):
	        dic_notnull=self.dic_notnull
	        null_data_list=[]
	        for col in tableColumns:
	            try:
	                if col in dic_notnull[table.replace('_tmp', '')]:
	                    notNullColumn=self.matchByConfig(table,col,order)
	                    null_data = tableData[tableData[notNullColumn].isnull() ]
	                    try:
	                        null_data=null_data.concat(tableData[tableData[notNullColumn] == '-'])
	                    except:
	                        Exception
	                    if len(null_data)!=0:
	                        null_data['comments']=np.array([col+' is null'] *len(null_data))
	                        null_data_list.append(null_data)
	                        tableData = tableData[~tableData[notNullColumn].isnull()]
	                        try:
	                            tableData = tableData[tableData[notNullColumn]!='-']
	                        except:
	                            Exception
	            except KeyError as e:
e5ddecd41a6823ad93bc9aeb41fd96d5|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|50|38|V|python
	    def filterNullData(self,tableData,tableColumns,table,order):
	        dic_notnull=self.dic_notnull
	        null_data_list=[]
	        for col in tableColumns:
	            try:
	                if col in dic_notnull[table.replace('_tmp', '')]:
	                    notNullColumn=self.matchByConfig(table,col,order)
	                    null_data = tableData[tableData[notNullColumn].isnull() ]
	                    try:
	                        null_data=null_data.concat(tableData[tableData[notNullColumn] == '-'])
	                    except:
	                        Exception
	                    if len(null_data)!=0:
	                        null_data['comments']=np.array([col+' is null'] *len(null_data))
	                        null_data_list.append(null_data)
	                        tableData = tableData[~tableData[notNullColumn].isnull()]
	                        try:
	                            tableData = tableData[tableData[notNullColumn]!='-']
	                        except:
	                            Exception
	            except KeyError as e:
	                print("Warning:not null info of table" + str(e) + "is missing! Please check your config ")
87c0b5af09ac4d36ac09241e8de50070|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|50|38|V|python
	    def filterNullData(self,tableData,tableColumns,table,order):
	        dic_notnull=self.dic_notnull
	        null_data_list=[]
	        for col in tableColumns:
	            try:
	                if col in dic_notnull[table.replace('_tmp', '')]:
	                    notNullColumn=self.matchByConfig(table,col,order)
	                    null_data = tableData[tableData[notNullColumn].isnull() ]
	                    try:
	                        null_data=null_data.concat(tableData[tableData[notNullColumn] == '-'])
	                    except:
	                        Exception
	                    if len(null_data)!=0:
	                        null_data['comments']=np.array([col+' is null'] *len(null_data))
	                        null_data_list.append(null_data)
	                        tableData = tableData[~tableData[notNullColumn].isnull()]
	                        try:
	                            tableData = tableData[tableData[notNullColumn]!='-']
	                        except:
	                            Exception
	            except KeyError as e:
	                print("Warning:not null info of table" + str(e) + "is missing! Please check your config ")
	                break
579879fb1fba41184a735f27623f472b|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|50|38|V|python
	    def filterNullData(self,tableData,tableColumns,table,order):
	        dic_notnull=self.dic_notnull
	        null_data_list=[]
	        for col in tableColumns:
	            try:
	                if col in dic_notnull[table.replace('_tmp', '')]:
	                    notNullColumn=self.matchByConfig(table,col,order)
	                    null_data = tableData[tableData[notNullColumn].isnull() ]
	                    try:
	                        null_data=null_data.concat(tableData[tableData[notNullColumn] == '-'])
	                    except:
	                        Exception
	                    if len(null_data)!=0:
	                        null_data['comments']=np.array([col+' is null'] *len(null_data))
	                        null_data_list.append(null_data)
	                        tableData = tableData[~tableData[notNullColumn].isnull()]
	                        try:
	                            tableData = tableData[tableData[notNullColumn]!='-']
	                        except:
	                            Exception
	            except KeyError as e:
	                print("Warning:not null info of table" + str(e) + "is missing! Please check your config ")
	                break
	        # ignore the case if all records are unqualified and the order is greater than 1
6cfe7952b586eed6db1b588a27ac11a6|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|50|38|V|python
	    def filterNullData(self,tableData,tableColumns,table,order):
	        dic_notnull=self.dic_notnull
	        null_data_list=[]
	        for col in tableColumns:
	            try:
	                if col in dic_notnull[table.replace('_tmp', '')]:
	                    notNullColumn=self.matchByConfig(table,col,order)
	                    null_data = tableData[tableData[notNullColumn].isnull() ]
	                    try:
	                        null_data=null_data.concat(tableData[tableData[notNullColumn] == '-'])
	                    except:
	                        Exception
	                    if len(null_data)!=0:
	                        null_data['comments']=np.array([col+' is null'] *len(null_data))
	                        null_data_list.append(null_data)
	                        tableData = tableData[~tableData[notNullColumn].isnull()]
	                        try:
	                            tableData = tableData[tableData[notNullColumn]!='-']
	                        except:
	                            Exception
	            except KeyError as e:
	                print("Warning:not null info of table" + str(e) + "is missing! Please check your config ")
	                break
	        # ignore the case if all records are unqualified and the order is greater than 1
	        if len(null_data_list)==len(tableData) and order>=1:
406136fd1fe1d087b76ea2a034e3ac8c|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|50|38|V|python
	    def filterNullData(self,tableData,tableColumns,table,order):
	        dic_notnull=self.dic_notnull
	        null_data_list=[]
	        for col in tableColumns:
	            try:
	                if col in dic_notnull[table.replace('_tmp', '')]:
	                    notNullColumn=self.matchByConfig(table,col,order)
	                    null_data = tableData[tableData[notNullColumn].isnull() ]
	                    try:
	                        null_data=null_data.concat(tableData[tableData[notNullColumn] == '-'])
	                    except:
	                        Exception
	                    if len(null_data)!=0:
	                        null_data['comments']=np.array([col+' is null'] *len(null_data))
	                        null_data_list.append(null_data)
	                        tableData = tableData[~tableData[notNullColumn].isnull()]
	                        try:
	                            tableData = tableData[tableData[notNullColumn]!='-']
	                        except:
	                            Exception
	            except KeyError as e:
	                print("Warning:not null info of table" + str(e) + "is missing! Please check your config ")
	                break
	        # ignore the case if all records are unqualified and the order is greater than 1
	        if len(null_data_list)==len(tableData) and order>=1:
	            return tableData
3743609f1d688923bcb35749baf006ea|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|50|38|V|python
	    def filterNullData(self,tableData,tableColumns,table,order):
	        dic_notnull=self.dic_notnull
	        null_data_list=[]
	        for col in tableColumns:
	            try:
	                if col in dic_notnull[table.replace('_tmp', '')]:
	                    notNullColumn=self.matchByConfig(table,col,order)
	                    null_data = tableData[tableData[notNullColumn].isnull() ]
	                    try:
	                        null_data=null_data.concat(tableData[tableData[notNullColumn] == '-'])
	                    except:
	                        Exception
	                    if len(null_data)!=0:
	                        null_data['comments']=np.array([col+' is null'] *len(null_data))
	                        null_data_list.append(null_data)
	                        tableData = tableData[~tableData[notNullColumn].isnull()]
	                        try:
	                            tableData = tableData[tableData[notNullColumn]!='-']
	                        except:
	                            Exception
	            except KeyError as e:
	                print("Warning:not null info of table" + str(e) + "is missing! Please check your config ")
	                break
	        # ignore the case if all records are unqualified and the order is greater than 1
	        if len(null_data_list)==len(tableData) and order>=1:
	            return tableData
	        # log the null records
ec264d4806491dd0ec070e835b17af25|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|50|38|V|python
	    def filterNullData(self,tableData,tableColumns,table,order):
	        dic_notnull=self.dic_notnull
	        null_data_list=[]
	        for col in tableColumns:
	            try:
	                if col in dic_notnull[table.replace('_tmp', '')]:
	                    notNullColumn=self.matchByConfig(table,col,order)
	                    null_data = tableData[tableData[notNullColumn].isnull() ]
	                    try:
	                        null_data=null_data.concat(tableData[tableData[notNullColumn] == '-'])
	                    except:
	                        Exception
	                    if len(null_data)!=0:
	                        null_data['comments']=np.array([col+' is null'] *len(null_data))
	                        null_data_list.append(null_data)
	                        tableData = tableData[~tableData[notNullColumn].isnull()]
	                        try:
	                            tableData = tableData[tableData[notNullColumn]!='-']
	                        except:
	                            Exception
	            except KeyError as e:
	                print("Warning:not null info of table" + str(e) + "is missing! Please check your config ")
	                break
	        # ignore the case if all records are unqualified and the order is greater than 1
	        if len(null_data_list)==len(tableData) and order>=1:
	            return tableData
	        # log the null records
	        if len(null_data_list)!=0:
13e2d99eabab068b24772edc923c4944|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|50|38|V|python
	    def filterNullData(self,tableData,tableColumns,table,order):
	        dic_notnull=self.dic_notnull
	        null_data_list=[]
	        for col in tableColumns:
	            try:
	                if col in dic_notnull[table.replace('_tmp', '')]:
	                    notNullColumn=self.matchByConfig(table,col,order)
	                    null_data = tableData[tableData[notNullColumn].isnull() ]
	                    try:
	                        null_data=null_data.concat(tableData[tableData[notNullColumn] == '-'])
	                    except:
	                        Exception
	                    if len(null_data)!=0:
	                        null_data['comments']=np.array([col+' is null'] *len(null_data))
	                        null_data_list.append(null_data)
	                        tableData = tableData[~tableData[notNullColumn].isnull()]
	                        try:
	                            tableData = tableData[tableData[notNullColumn]!='-']
	                        except:
	                            Exception
	            except KeyError as e:
	                print("Warning:not null info of table" + str(e) + "is missing! Please check your config ")
	                break
	        # ignore the case if all records are unqualified and the order is greater than 1
	        if len(null_data_list)==len(tableData) and order>=1:
	            return tableData
	        # log the null records
	        if len(null_data_list)!=0:
	            null_dataset = pd.concat(null_data_list)
3497d5c9005a9697b36bf86635f2f4f3|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|50|38|V|python
	    def filterNullData(self,tableData,tableColumns,table,order):
	        dic_notnull=self.dic_notnull
	        null_data_list=[]
	        for col in tableColumns:
	            try:
	                if col in dic_notnull[table.replace('_tmp', '')]:
	                    notNullColumn=self.matchByConfig(table,col,order)
	                    null_data = tableData[tableData[notNullColumn].isnull() ]
	                    try:
	                        null_data=null_data.concat(tableData[tableData[notNullColumn] == '-'])
	                    except:
	                        Exception
	                    if len(null_data)!=0:
	                        null_data['comments']=np.array([col+' is null'] *len(null_data))
	                        null_data_list.append(null_data)
	                        tableData = tableData[~tableData[notNullColumn].isnull()]
	                        try:
	                            tableData = tableData[tableData[notNullColumn]!='-']
	                        except:
	                            Exception
	            except KeyError as e:
	                print("Warning:not null info of table" + str(e) + "is missing! Please check your config ")
	                break
	        # ignore the case if all records are unqualified and the order is greater than 1
	        if len(null_data_list)==len(tableData) and order>=1:
	            return tableData
	        # log the null records
	        if len(null_data_list)!=0:
	            null_dataset = pd.concat(null_data_list)
	            table=table.replace('_tmp','')
a4ec962d249be3e9606d8293769233e9|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|50|38|V|python
	    def filterNullData(self,tableData,tableColumns,table,order):
	        dic_notnull=self.dic_notnull
	        null_data_list=[]
	        for col in tableColumns:
	            try:
	                if col in dic_notnull[table.replace('_tmp', '')]:
	                    notNullColumn=self.matchByConfig(table,col,order)
	                    null_data = tableData[tableData[notNullColumn].isnull() ]
	                    try:
	                        null_data=null_data.concat(tableData[tableData[notNullColumn] == '-'])
	                    except:
	                        Exception
	                    if len(null_data)!=0:
	                        null_data['comments']=np.array([col+' is null'] *len(null_data))
	                        null_data_list.append(null_data)
	                        tableData = tableData[~tableData[notNullColumn].isnull()]
	                        try:
	                            tableData = tableData[tableData[notNullColumn]!='-']
	                        except:
	                            Exception
	            except KeyError as e:
	                print("Warning:not null info of table" + str(e) + "is missing! Please check your config ")
	                break
	        # ignore the case if all records are unqualified and the order is greater than 1
	        if len(null_data_list)==len(tableData) and order>=1:
	            return tableData
	        # log the null records
	        if len(null_data_list)!=0:
	            null_dataset = pd.concat(null_data_list)
	            table=table.replace('_tmp','')
	            if table.find('data')>=0:
eaf0f9094b709dc2f75c8d786af21c24|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|50|38|V|python
	    def filterNullData(self,tableData,tableColumns,table,order):
	        dic_notnull=self.dic_notnull
	        null_data_list=[]
	        for col in tableColumns:
	            try:
	                if col in dic_notnull[table.replace('_tmp', '')]:
	                    notNullColumn=self.matchByConfig(table,col,order)
	                    null_data = tableData[tableData[notNullColumn].isnull() ]
	                    try:
	                        null_data=null_data.concat(tableData[tableData[notNullColumn] == '-'])
	                    except:
	                        Exception
	                    if len(null_data)!=0:
	                        null_data['comments']=np.array([col+' is null'] *len(null_data))
	                        null_data_list.append(null_data)
	                        tableData = tableData[~tableData[notNullColumn].isnull()]
	                        try:
	                            tableData = tableData[tableData[notNullColumn]!='-']
	                        except:
	                            Exception
	            except KeyError as e:
	                print("Warning:not null info of table" + str(e) + "is missing! Please check your config ")
	                break
	        # ignore the case if all records are unqualified and the order is greater than 1
	        if len(null_data_list)==len(tableData) and order>=1:
	            return tableData
	        # log the null records
	        if len(null_data_list)!=0:
	            null_dataset = pd.concat(null_data_list)
	            table=table.replace('_tmp','')
	            if table.find('data')>=0:
	                errorFileName =str(datetime.datetime.now())+'_'+str(self.fileName) + "_" + table + "_"+ str(order + 1) + '_not_null.csv'
e28a15a359f2a2b2c3ff52ee868ced77|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|50|38|V|python
	    def filterNullData(self,tableData,tableColumns,table,order):
	        dic_notnull=self.dic_notnull
	        null_data_list=[]
	        for col in tableColumns:
	            try:
	                if col in dic_notnull[table.replace('_tmp', '')]:
	                    notNullColumn=self.matchByConfig(table,col,order)
	                    null_data = tableData[tableData[notNullColumn].isnull() ]
	                    try:
	                        null_data=null_data.concat(tableData[tableData[notNullColumn] == '-'])
	                    except:
	                        Exception
	                    if len(null_data)!=0:
	                        null_data['comments']=np.array([col+' is null'] *len(null_data))
	                        null_data_list.append(null_data)
	                        tableData = tableData[~tableData[notNullColumn].isnull()]
	                        try:
	                            tableData = tableData[tableData[notNullColumn]!='-']
	                        except:
	                            Exception
	            except KeyError as e:
	                print("Warning:not null info of table" + str(e) + "is missing! Please check your config ")
	                break
	        # ignore the case if all records are unqualified and the order is greater than 1
	        if len(null_data_list)==len(tableData) and order>=1:
	            return tableData
	        # log the null records
	        if len(null_data_list)!=0:
	            null_dataset = pd.concat(null_data_list)
	            table=table.replace('_tmp','')
	            if table.find('data')>=0:
	                errorFileName =str(datetime.datetime.now())+'_'+str(self.fileName) + "_" + table + "_"+ str(order + 1) + '_not_null.csv'
	            else:
4e4bc1897c96d58c6f50251244ae8d57|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|50|38|V|python
	    def filterNullData(self,tableData,tableColumns,table,order):
	        dic_notnull=self.dic_notnull
	        null_data_list=[]
	        for col in tableColumns:
	            try:
	                if col in dic_notnull[table.replace('_tmp', '')]:
	                    notNullColumn=self.matchByConfig(table,col,order)
	                    null_data = tableData[tableData[notNullColumn].isnull() ]
	                    try:
	                        null_data=null_data.concat(tableData[tableData[notNullColumn] == '-'])
	                    except:
	                        Exception
	                    if len(null_data)!=0:
	                        null_data['comments']=np.array([col+' is null'] *len(null_data))
	                        null_data_list.append(null_data)
	                        tableData = tableData[~tableData[notNullColumn].isnull()]
	                        try:
	                            tableData = tableData[tableData[notNullColumn]!='-']
	                        except:
	                            Exception
	            except KeyError as e:
	                print("Warning:not null info of table" + str(e) + "is missing! Please check your config ")
	                break
	        # ignore the case if all records are unqualified and the order is greater than 1
	        if len(null_data_list)==len(tableData) and order>=1:
	            return tableData
	        # log the null records
	        if len(null_data_list)!=0:
	            null_dataset = pd.concat(null_data_list)
	            table=table.replace('_tmp','')
	            if table.find('data')>=0:
	                errorFileName =str(datetime.datetime.now())+'_'+str(self.fileName) + "_" + table + "_"+ str(order + 1) + '_not_null.csv'
	            else:
	                errorFileName = str(datetime.datetime.now())+'_'+str(self.fileName) + "_" + table + '_not_null.csv'
44284a640f15b8ae787bf76ed5b532f9|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|50|38|V|python
	    def filterNullData(self,tableData,tableColumns,table,order):
	        dic_notnull=self.dic_notnull
	        null_data_list=[]
	        for col in tableColumns:
	            try:
	                if col in dic_notnull[table.replace('_tmp', '')]:
	                    notNullColumn=self.matchByConfig(table,col,order)
	                    null_data = tableData[tableData[notNullColumn].isnull() ]
	                    try:
	                        null_data=null_data.concat(tableData[tableData[notNullColumn] == '-'])
	                    except:
	                        Exception
	                    if len(null_data)!=0:
	                        null_data['comments']=np.array([col+' is null'] *len(null_data))
	                        null_data_list.append(null_data)
	                        tableData = tableData[~tableData[notNullColumn].isnull()]
	                        try:
	                            tableData = tableData[tableData[notNullColumn]!='-']
	                        except:
	                            Exception
	            except KeyError as e:
	                print("Warning:not null info of table" + str(e) + "is missing! Please check your config ")
	                break
	        # ignore the case if all records are unqualified and the order is greater than 1
	        if len(null_data_list)==len(tableData) and order>=1:
	            return tableData
	        # log the null records
	        if len(null_data_list)!=0:
	            null_dataset = pd.concat(null_data_list)
	            table=table.replace('_tmp','')
	            if table.find('data')>=0:
	                errorFileName =str(datetime.datetime.now())+'_'+str(self.fileName) + "_" + table + "_"+ str(order + 1) + '_not_null.csv'
	            else:
	                errorFileName = str(datetime.datetime.now())+'_'+str(self.fileName) + "_" + table + '_not_null.csv'
	            self.logErrorRecord(null_dataset, errorFileName)
a6bf8c80bd3560d28460d3a7f37f76eb|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|50|38|V|python
	    def filterNullData(self,tableData,tableColumns,table,order):
	        dic_notnull=self.dic_notnull
	        null_data_list=[]
	        for col in tableColumns:
	            try:
	                if col in dic_notnull[table.replace('_tmp', '')]:
	                    notNullColumn=self.matchByConfig(table,col,order)
	                    null_data = tableData[tableData[notNullColumn].isnull() ]
	                    try:
	                        null_data=null_data.concat(tableData[tableData[notNullColumn] == '-'])
	                    except:
	                        Exception
	                    if len(null_data)!=0:
	                        null_data['comments']=np.array([col+' is null'] *len(null_data))
	                        null_data_list.append(null_data)
	                        tableData = tableData[~tableData[notNullColumn].isnull()]
	                        try:
	                            tableData = tableData[tableData[notNullColumn]!='-']
	                        except:
	                            Exception
	            except KeyError as e:
	                print("Warning:not null info of table" + str(e) + "is missing! Please check your config ")
	                break
	        # ignore the case if all records are unqualified and the order is greater than 1
	        if len(null_data_list)==len(tableData) and order>=1:
	            return tableData
	        # log the null records
	        if len(null_data_list)!=0:
	            null_dataset = pd.concat(null_data_list)
	            table=table.replace('_tmp','')
	            if table.find('data')>=0:
	                errorFileName =str(datetime.datetime.now())+'_'+str(self.fileName) + "_" + table + "_"+ str(order + 1) + '_not_null.csv'
	            else:
	                errorFileName = str(datetime.datetime.now())+'_'+str(self.fileName) + "_" + table + '_not_null.csv'
	            self.logErrorRecord(null_dataset, errorFileName)
	        return tableData
ef888ed372fdf46482f9c21a2c7380ae|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|213|1|V|python
	    def matchByConfig(self,table,col,order):
	        dic_match = self.dic_match
	        try:
	            matchedCols = dic_match[table.replace('_tmp', '') + '.' + col]
	        except KeyError as key:
	            print("Warning:can't find column", str(key), "! Please check your config.")
	            return False
	        if order > len(matchedCols) - 1:
	            matchedCol = matchedCols[0]
	        else:
	            matchedCol = matchedCols[order]
	        return matchedCol
bfbd89e7ab2bde085dc48160015e303f|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|501|1|V|python
	        self.pandasData = self.filterNullData(self.pandasData, self.tableColumns[0], self.table[0], order=0)
02252528dc67a47a4ac4771c8d31454e|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|450|1|V|python
	    def drop_duplicates(self,tableData):
	        return tableData.drop_duplicates()
cd9c9eb634a7cb0dd13a2479325b60c8|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|157|1|V|python
	    def genSqlHeader(self):
	        self.sqlHeaders = [""] * len(self.table)
	        i=0
	        for table in self.table:
	            colnames = str(self.tableColumns[i]).replace('[', '(').replace(']', ')').replace("'", "")
	            self.sqlHeaders[i] = "insert into {0}.{1}{2} values".format(self.schema, table, colnames)
	            i=i+1
	        return self.sqlHeaders
a4a20ac96f37ef68ba1f0209bbf8fd76|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill_service_dal.py|60|1|V|python
	        Keyword arguments:
	            schema -- the tmp schema, not the real one
	            table_tmp -- the tmp table, not real table
96bb6af74f18a6007ddfb30508a232f0|file:///Users/jerry/repo/ibmeww-etl/script/common/base_loader.py|439|1|V|python
	        Keyword arguments:
	            table_names -- the table names you get from configuration, it could
	                        be only one, or two, one with index, and then other with data
2c144fd0893f9d3e648d23f1a2cb1d92|file:///Users/jerry/repo/ibmeww-etl/script/common/base_loader.py|430|1|V|python
	        Keyword arguments:
	            panda_table_data -- the pandas data frame you want to remove
	                            duplicates
e322e83de098f95c47f46d07688d2734|file:///Users/jerry/repo/ibmeww-etl/script/common/base_loader.py|407|1|V|python
	                errorFileName = str(datetime.datetime.now()) + '_' + \
	                                str(self.input_file) + "_" + \
	                                table_name + "_"+ str(order + 1) + '_not_null.csv'
d5ac4a6a35b151848b44af95e4d19b01|file:///Users/jerry/repo/ibmeww-etl/script/common/base_loader.py|406|16|v|python
	table_name.find('data') >= 0
d6bb1daf6f678e652cd5e6f90ef9cee9|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill_service_dal.py|32|1|V|python
	NECESSARY_FILES_SUBFIX=("AltMin.csv","Assay.csv","Collar.csv","Event.csv",
	                        "IntervalStructure.csv","PointStructure.csv","RockDesc.csv",
	                        "RQD.csv","Survey.csv", "Terraspec.csv")
78dd2f00043e6046a6151863f7635550|file:///Users/jerry/repo/ibmeww-etl/config/config_dev.json|437|1|V|json
			"VeinsTable1.csv":["drill_temp.veindata_tmp"],
76c7126ea0cea61f6113274f8728128c|file:///Users/jerry/repo/ibmeww-etl/config/config_dev.json|438|22|v|json
	"drill_temp.veinindex_tmp"
08b4b82c5e48997a70d7d36163161106|file:///Users/jerry/repo/ibmeww-etl/config/config_dev.json|132|15|v|json
	mineral
37418eeeb4639fee6857474b1469407a|file:///Users/jerry/repo/ibmeww-etl/config/config_dev.json|133|15|v|json
	mineral_count
a0f3ee1f384185abb4afcbf09d9784c7|file:///Users/jerry/repo/ibmeww-etl/script/common/base_loader.py|177|1|V|python
	        print('project code is {}'.format(project_code))
73489b84c72a302793a50a5e66be7dc1|file:///Users/jerry/repo/ibmeww-etl/config/config_dev.json|63|1|V|json
	    "soilindex.sampleid": [""],
10acebf3c4ff475ab264a88456580fab|file:///Users/jerry/repo/ibmeww-etl/config/config_dev.json|71|1|V|json
	    "soilindex.landuse": [""],
a29e385334b91729624abddf50be0dd5|file:///Users/jerry/repo/ibmeww-etl/config/config_dev.json|79|1|V|json
	    "soilindex.rf_pct": [""],
58a9b97c2f61d89b6af26cce92fade00|file:///Users/jerry/repo/ibmeww-etl/config/config_dev.json|89|1|V|json
	    "soildata.chemical_element": [""],
4cd78105adf9fd91e61206155ff2740e|file:///Users/jerry/repo/ibmeww-etl/config/config_dev.json|394|17|v|json
	chemical_element
f5ce31911209013c86bdb3ecd82fbabe|file:///Users/jerry/repo/ibmeww-etl/config/config_dev.json|394|1|V|json
			"assaydata":["chemical_element", "measurement_unit", "measurement_value"],
86e881407dbdda2c7bb2691f8409f305|file:///Users/jerry/repo/ibmeww-etl/config/config_dev.json|90|15|v|json
	measurement_unit
ea8075b3a612a74b288ae732f7170175|file:///Users/jerry/repo/ibmeww-etl/config/config_dev.json|91|15|v|json
	measurement_value
8be50d8f569aab2c51c9c6cbf84e05f3|file:///Users/jerry/repo/ibmeww-etl/config/config_dev.json|474|1|V|json
			"veinindex":["veindata"],
0000833150ff499a43ed91764c96b59a|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|514|1|V|python
	        # 3.Fetch data from data frame for each table
	        # 3.1 get the split number of each table
ed9ef7e989a4429c2764b65d6bea1572|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|516|1|V|python
	        for i,table in enumerate(self.table):
8eaccfebf27ee12f505e80c541ba2a0a|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|516|1|V|python
	            print('*******************',table,'************************')
8d823c6cb5309ab7d3131eea833866e2|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|516|1|V|python
	            print('*******************',table,'************************')
	            numSplit = self.numSplit[i]
fffa440a6b164f2900cf9f162f3e452c|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|516|1|V|python
	            print('*******************',table,'************************')
	            numSplit = self.numSplit[i]
	            tableColumns=self.tableColumns[i]
4e741a33d64da94cfe28d61994f307d7|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|516|1|V|python
	            print('*******************',table,'************************')
	            numSplit = self.numSplit[i]
	            tableColumns=self.tableColumns[i]
	            # 3.2 fetch data for each split table
bb365796634de0066598f1ed886edc3c|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|516|1|V|python
	            print('*******************',table,'************************')
	            numSplit = self.numSplit[i]
	            tableColumns=self.tableColumns[i]
	            # 3.2 fetch data for each split table
	            for order in range(0, numSplit):
96deefc6472491d20562986e79594995|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|521|1|V|python
	            for order in range(0, numSplit):
	                print('******************* order=', order, '*******************')
	                tableData=self.fetchFromFile(table,order,tableColumns)
	                # 4. Filter records for each table
	                # 4.1 filter duplicate records
	                tableData = self.drop_duplicates(tableData)
	                # 4.2 filter records that violate not null constraint
	                if i!=0:
	                    tableData = self.filterNullData(tableData,tableColumns,table,order)
	                # 5. Insert records to targert db by batch, here we generate sql manually
	                self.sqlHeaders = self.genSqlHeader()
	                try:
	                    for date_column in self.date_columns:
	                        if date_column in list(self.pandasData.columns.values):
	                            self.unify_datetype_ymd(tableData,date_column=date_column)
	                except Exception as e:
	                    print(e)
	                self.insertByBatch(tableData,self.sqlHeaders[i])
4a06d26a1966de62c4a7f24117b21c33|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|226|1|V|python
	    def fetchFromFile(self,table,order,tableColumns):
	        self.fileCols = []
	        # 1. judge whether current column is in column list of dataset
	        for col in tableColumns:
	            if col in self.pandasData.columns and col.find("order")<0:
	                self.fileCols.append(col)
	            else:
	                fileCol = self.matchByConfig(table, col, order)
	                if fileCol!=False:
	                # 2. match columns between file and table by configured dictionary
	                # if matched column is null,we generate nan column for it
	                    if fileCol=='':
	                        fileCol=col+'_null'
	                        self.pandasData[fileCol]=np.array(float('nan') * self.numRawRecords)
	                    if fileCol in self.pandasData.columns:
	                        self.fileCols.append(fileCol)
	                    elif fileCol in ['chemical_element', 'measurement_unit',
	                            'measurement_value']:
	                        self.fileCols.append(fileCol)
	                else:
	                # if current column is missing ,generate values for it.
	                    self.genHiddenCol(col,order)
	        # 3. select columns for current table
	        print("-----1----------")
	        print(self.fileCols)
	        print(self.pandasData.columns)
	        print("-----2----------")
	        if table == 'assaydata_tmp':
	            tableData = self.__getAssayData()
	        else:
	            tableData = self.pandasData[self.fileCols]
	        return tableData
	
25855144b595faf922b23834741bcf8d|file:///Users/jerry/repo/ibmeww-etl/script/surface/surface.py|105|1|V|python
	                self.sqlHeaders = self.generate_sql_header(table_names, table_columns)
3ead355770c2868cce66dffe40f3e65a|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|384|1|V|python
	    def insertByBatch(self,tableData,sqlHeader):
	        # 4.1 divide the records into batches
	        numRecords = len(tableData)
	        batchSize = self.batchSize
	        numBatch = int(math.ceil(float(numRecords) / batchSize))
	        for numInsert in range(numBatch):
	            t1=datetime.datetime.now()
	            print("batch ",numInsert," start inserting")
	            numStart = numInsert * batchSize
	            if (numInsert + 1) * batchSize > numRecords:
	                numEnd = numRecords
	            else:
	                numEnd = (numInsert + 1) * batchSize
	            batchData = tableData[numStart:numEnd]
	        # 4.2 generate sql values manully
	            sqlValues=self.genSqlValues(batchData)
	            sql = sqlHeader + sqlValues
	            print(sql.split('),(')[0]+')')
	        # # 4.3 execute sql
	            try:
	                self.pgsql.query(sql,reply=False)
	                self.pgsql.commit()
	            except Exception as e:
	                traceback.print_exc()
	                raise gip_exception.GIPException(e)
	            print("batch ", numInsert, " end inserting")
	            t2 = datetime.datetime.now()
	            print("time comsuming: ", t2-t1)
	
d511c93b9b902f582aea38cbc5ef6e08|file:///Users/jerry/repo/ibmeww-etl/script/common/base_service.py|55|1|V|python
	        Keyword arguments:
	            commit -- to commit your query or not, default to True
ffa04217f44e4a2773e75640c9d63ad8|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|18|1|V|python
	import math
e1d0fa0e1d1d0800742d95d4cdca3270|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|369|1|V|python
	    def genSqlValues(self,batchData):
	        sqlValue = []
	        for record in batchData.values:
	            values = []
	            for value in record:
	                if value=='"'or value=='-':
	                    value="null"
	                elif type(value) == str :#todo value.find('ST_')<0 ?
	                    value = "'" + value.replace("'", "''")+ "'"
	                values.append(str(value))
	            values = '(' + ','.join(values) + ')'
	            sqlValue.append(values.replace('nan', 'null').replace('None','null').replace("'null'",'null'))
	        sqlValues = ','.join(sqlValue)
	        return sqlValues
e23b70207c3c81677ff333ee39af4c91|file:///Users/jerry/repo/ibmeww-etl/script/common/base_loader.py|465|1|V|python
	        Keyword arguments:
	            table_names -- the table names you get from configuration, it could
7e4732c49ef370c80a8a64dfbbffe819|file:///Users/jerry/repo/ibmeww-etl/script/common/base_loader.py|533|1|V|python
	                self.pgsql.query(sql,reply=False)
884ede7340139fadbfdab34b1e5e74a8|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|507|1|V|python
	        self.numRawRecords = len(self.pandasData)
837a5e2391913a21d80b25a9afa3d1bc|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|507|1|V|python
	    def genHiddenCol(self,col,order):
93aca7332d0162a6cfce5cfbe9bcf65f|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|507|1|V|python
	    def genHiddenCol(self,col,order):
	        self.fileCols.append(col)
1d7da43da08a99a76a8e636d69b49956|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|507|1|V|python
	    def genHiddenCol(self,col,order):
	        self.fileCols.append(col)
	        # Hidden column can be vary from each file, just inherit GC_loader_split class and overwrite isHiddenCol function
792b98cbc8ebe9e75d2bc6d7fe2ae9aa|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|507|1|V|python
	    def genHiddenCol(self,col,order):
	        self.fileCols.append(col)
	        # Hidden column can be vary from each file, just inherit GC_loader_split class and overwrite isHiddenCol function
	        if col.find('_intervalid')>=0 :
5eb110514e6b0dd9e38da54c2783ac37|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|507|1|V|python
	    def genHiddenCol(self,col,order):
	        self.fileCols.append(col)
	        # Hidden column can be vary from each file, just inherit GC_loader_split class and overwrite isHiddenCol function
	        if col.find('_intervalid')>=0 :
	            self.pandasData[col] = np.arange(start=self.seq, stop=self.seq + self.numRawRecords, step=1)
09a7d05075d37f692b4c2ff332ebdaa4|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|507|1|V|python
	    def genHiddenCol(self,col,order):
	        self.fileCols.append(col)
	        # Hidden column can be vary from each file, just inherit GC_loader_split class and overwrite isHiddenCol function
	        if col.find('_intervalid')>=0 :
	            self.pandasData[col] = np.arange(start=self.seq, stop=self.seq + self.numRawRecords, step=1)
	        elif col == 'projectid':
3116dbc8b7904b765604b5c227c39a94|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|507|1|V|python
	    def genHiddenCol(self,col,order):
	        self.fileCols.append(col)
	        # Hidden column can be vary from each file, just inherit GC_loader_split class and overwrite isHiddenCol function
	        if col.find('_intervalid')>=0 :
	            self.pandasData[col] = np.arange(start=self.seq, stop=self.seq + self.numRawRecords, step=1)
	        elif col == 'projectid':
	            self.pandasData[col] = np.array([self.projectid] * self.numRawRecords)
10f31045c19ab6b96a402c30a13663e6|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|507|1|V|python
	    def genHiddenCol(self,col,order):
	        self.fileCols.append(col)
	        # Hidden column can be vary from each file, just inherit GC_loader_split class and overwrite isHiddenCol function
	        if col.find('_intervalid')>=0 :
	            self.pandasData[col] = np.arange(start=self.seq, stop=self.seq + self.numRawRecords, step=1)
	        elif col == 'projectid':
	            self.pandasData[col] = np.array([self.projectid] * self.numRawRecords)
	        elif col.find('order') >= 0:
8e177a487990d2f4530c04365186a87b|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|507|1|V|python
	    def genHiddenCol(self,col,order):
	        self.fileCols.append(col)
	        # Hidden column can be vary from each file, just inherit GC_loader_split class and overwrite isHiddenCol function
	        if col.find('_intervalid')>=0 :
	            self.pandasData[col] = np.arange(start=self.seq, stop=self.seq + self.numRawRecords, step=1)
	        elif col == 'projectid':
	            self.pandasData[col] = np.array([self.projectid] * self.numRawRecords)
	        elif col.find('order') >= 0:
	            self.pandasData[col] = np.array([order + 1] * self.numRawRecords)
5a0eb67ecfa8f733ccc669278c7c58eb|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|507|1|V|python
	    def genHiddenCol(self,col,order):
	        self.fileCols.append(col)
	        # Hidden column can be vary from each file, just inherit GC_loader_split class and overwrite isHiddenCol function
	        if col.find('_intervalid')>=0 :
	            self.pandasData[col] = np.arange(start=self.seq, stop=self.seq + self.numRawRecords, step=1)
	        elif col == 'projectid':
	            self.pandasData[col] = np.array([self.projectid] * self.numRawRecords)
	        elif col.find('order') >= 0:
	            self.pandasData[col] = np.array([order + 1] * self.numRawRecords)
	        elif self.table[0].find('collar')>=0 and col.find('dhid')>=0:
ba63cb7a2de938021c5a51f09f5dac32|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|507|1|V|python
	    def genHiddenCol(self,col,order):
	        self.fileCols.append(col)
	        # Hidden column can be vary from each file, just inherit GC_loader_split class and overwrite isHiddenCol function
	        if col.find('_intervalid')>=0 :
	            self.pandasData[col] = np.arange(start=self.seq, stop=self.seq + self.numRawRecords, step=1)
	        elif col == 'projectid':
	            self.pandasData[col] = np.array([self.projectid] * self.numRawRecords)
	        elif col.find('order') >= 0:
	            self.pandasData[col] = np.array([order + 1] * self.numRawRecords)
	        elif self.table[0].find('collar')>=0 and col.find('dhid')>=0:
	            self.pandasData[col] = np.arange(start=self.seq, stop=self.seq + self.numRawRecords, step=1)
0d2f475adc11ab82b1517d8affc93c31|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|507|1|V|python
	    def genHiddenCol(self,col,order):
	        self.fileCols.append(col)
	        # Hidden column can be vary from each file, just inherit GC_loader_split class and overwrite isHiddenCol function
	        if col.find('_intervalid')>=0 :
	            self.pandasData[col] = np.arange(start=self.seq, stop=self.seq + self.numRawRecords, step=1)
	        elif col == 'projectid':
	            self.pandasData[col] = np.array([self.projectid] * self.numRawRecords)
	        elif col.find('order') >= 0:
	            self.pandasData[col] = np.array([order + 1] * self.numRawRecords)
	        elif self.table[0].find('collar')>=0 and col.find('dhid')>=0:
	            self.pandasData[col] = np.arange(start=self.seq, stop=self.seq + self.numRawRecords, step=1)
	        elif self.table[0].find('collar')>=0 and col.find('point')>=0:
b07cbdab99b390c0f4efd5ba09c6ab5e|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|507|1|V|python
	    def genHiddenCol(self,col,order):
	        self.fileCols.append(col)
	        # Hidden column can be vary from each file, just inherit GC_loader_split class and overwrite isHiddenCol function
	        if col.find('_intervalid')>=0 :
	            self.pandasData[col] = np.arange(start=self.seq, stop=self.seq + self.numRawRecords, step=1)
	        elif col == 'projectid':
	            self.pandasData[col] = np.array([self.projectid] * self.numRawRecords)
	        elif col.find('order') >= 0:
	            self.pandasData[col] = np.array([order + 1] * self.numRawRecords)
	        elif self.table[0].find('collar')>=0 and col.find('dhid')>=0:
	            self.pandasData[col] = np.arange(start=self.seq, stop=self.seq + self.numRawRecords, step=1)
	        elif self.table[0].find('collar')>=0 and col.find('point')>=0:
	            self.pandasData[col] = np.array(['01010000A0417D0000000000000000000000000000000000000000000000000000'] * self.numRawRecords)
f70ae09984d7d59ae6510990299eeb2b|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|507|1|V|python
	    def genHiddenCol(self,col,order):
	        self.fileCols.append(col)
	        # Hidden column can be vary from each file, just inherit GC_loader_split class and overwrite isHiddenCol function
	        if col.find('_intervalid')>=0 :
	            self.pandasData[col] = np.arange(start=self.seq, stop=self.seq + self.numRawRecords, step=1)
	        elif col == 'projectid':
	            self.pandasData[col] = np.array([self.projectid] * self.numRawRecords)
	        elif col.find('order') >= 0:
	            self.pandasData[col] = np.array([order + 1] * self.numRawRecords)
	        elif self.table[0].find('collar')>=0 and col.find('dhid')>=0:
	            self.pandasData[col] = np.arange(start=self.seq, stop=self.seq + self.numRawRecords, step=1)
	        elif self.table[0].find('collar')>=0 and col.find('point')>=0:
	            self.pandasData[col] = np.array(['01010000A0417D0000000000000000000000000000000000000000000000000000'] * self.numRawRecords)
	
ef4c204b00af6404ca628a6fee21999a|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|507|1|V|python
	    def genHiddenCol(self,col,order):
	        self.fileCols.append(col)
	        # Hidden column can be vary from each file, just inherit GC_loader_split class and overwrite isHiddenCol function
	        if col.find('_intervalid')>=0 :
	            self.pandasData[col] = np.arange(start=self.seq, stop=self.seq + self.numRawRecords, step=1)
	        elif col == 'projectid':
	            self.pandasData[col] = np.array([self.projectid] * self.numRawRecords)
	        elif col.find('order') >= 0:
	            self.pandasData[col] = np.array([order + 1] * self.numRawRecords)
	        elif self.table[0].find('collar')>=0 and col.find('dhid')>=0:
	            self.pandasData[col] = np.arange(start=self.seq, stop=self.seq + self.numRawRecords, step=1)
	        elif self.table[0].find('collar')>=0 and col.find('point')>=0:
	            self.pandasData[col] = np.array(['01010000A0417D0000000000000000000000000000000000000000000000000000'] * self.numRawRecords)
	
	
b4a617d42cc2f81b1a968472dcda1e74|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|507|1|V|python
	    def genHiddenCol(self,col,order):
	        self.fileCols.append(col)
	        # Hidden column can be vary from each file, just inherit GC_loader_split class and overwrite isHiddenCol function
	        if col.find('_intervalid')>=0 :
	            self.pandasData[col] = np.arange(start=self.seq, stop=self.seq + self.numRawRecords, step=1)
	        elif col == 'projectid':
	            self.pandasData[col] = np.array([self.projectid] * self.numRawRecords)
	        elif col.find('order') >= 0:
	            self.pandasData[col] = np.array([order + 1] * self.numRawRecords)
	        elif self.table[0].find('collar')>=0 and col.find('dhid')>=0:
	            self.pandasData[col] = np.arange(start=self.seq, stop=self.seq + self.numRawRecords, step=1)
	        elif self.table[0].find('collar')>=0 and col.find('point')>=0:
	            self.pandasData[col] = np.array(['01010000A0417D0000000000000000000000000000000000000000000000000000'] * self.numRawRecords)
	
	
	        elif self.table[0].find('survey')>=0 and col.find('x')>=0:
56a2a92f2e8a45397d5e56b895b4a33a|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|507|1|V|python
	    def genHiddenCol(self,col,order):
	        self.fileCols.append(col)
	        # Hidden column can be vary from each file, just inherit GC_loader_split class and overwrite isHiddenCol function
	        if col.find('_intervalid')>=0 :
	            self.pandasData[col] = np.arange(start=self.seq, stop=self.seq + self.numRawRecords, step=1)
	        elif col == 'projectid':
	            self.pandasData[col] = np.array([self.projectid] * self.numRawRecords)
	        elif col.find('order') >= 0:
	            self.pandasData[col] = np.array([order + 1] * self.numRawRecords)
	        elif self.table[0].find('collar')>=0 and col.find('dhid')>=0:
	            self.pandasData[col] = np.arange(start=self.seq, stop=self.seq + self.numRawRecords, step=1)
	        elif self.table[0].find('collar')>=0 and col.find('point')>=0:
	            self.pandasData[col] = np.array(['01010000A0417D0000000000000000000000000000000000000000000000000000'] * self.numRawRecords)
	
	
	        elif self.table[0].find('survey')>=0 and col.find('x')>=0:
	            self.pandasData[col] = np.array([0.0] * self.numRawRecords)
1ab488ba6c4f608c36b82e10bb3fb0ca|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|507|1|V|python
	    def genHiddenCol(self,col,order):
	        self.fileCols.append(col)
	        # Hidden column can be vary from each file, just inherit GC_loader_split class and overwrite isHiddenCol function
	        if col.find('_intervalid')>=0 :
	            self.pandasData[col] = np.arange(start=self.seq, stop=self.seq + self.numRawRecords, step=1)
	        elif col == 'projectid':
	            self.pandasData[col] = np.array([self.projectid] * self.numRawRecords)
	        elif col.find('order') >= 0:
	            self.pandasData[col] = np.array([order + 1] * self.numRawRecords)
	        elif self.table[0].find('collar')>=0 and col.find('dhid')>=0:
	            self.pandasData[col] = np.arange(start=self.seq, stop=self.seq + self.numRawRecords, step=1)
	        elif self.table[0].find('collar')>=0 and col.find('point')>=0:
	            self.pandasData[col] = np.array(['01010000A0417D0000000000000000000000000000000000000000000000000000'] * self.numRawRecords)
	
	
	        elif self.table[0].find('survey')>=0 and col.find('x')>=0:
	            self.pandasData[col] = np.array([0.0] * self.numRawRecords)
	        elif self.table[0].find('survey')>=0 and col.find('y')>=0:
510af5afda0a12443c338fb81ce81aad|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|507|1|V|python
	    def genHiddenCol(self,col,order):
	        self.fileCols.append(col)
	        # Hidden column can be vary from each file, just inherit GC_loader_split class and overwrite isHiddenCol function
	        if col.find('_intervalid')>=0 :
	            self.pandasData[col] = np.arange(start=self.seq, stop=self.seq + self.numRawRecords, step=1)
	        elif col == 'projectid':
	            self.pandasData[col] = np.array([self.projectid] * self.numRawRecords)
	        elif col.find('order') >= 0:
	            self.pandasData[col] = np.array([order + 1] * self.numRawRecords)
	        elif self.table[0].find('collar')>=0 and col.find('dhid')>=0:
	            self.pandasData[col] = np.arange(start=self.seq, stop=self.seq + self.numRawRecords, step=1)
	        elif self.table[0].find('collar')>=0 and col.find('point')>=0:
	            self.pandasData[col] = np.array(['01010000A0417D0000000000000000000000000000000000000000000000000000'] * self.numRawRecords)
	
	
	        elif self.table[0].find('survey')>=0 and col.find('x')>=0:
	            self.pandasData[col] = np.array([0.0] * self.numRawRecords)
	        elif self.table[0].find('survey')>=0 and col.find('y')>=0:
	            self.pandasData[col] = np.array([0.0] * self.numRawRecords)
7838a74102e9961f0a4c7499103041f3|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|507|1|V|python
	    def genHiddenCol(self,col,order):
	        self.fileCols.append(col)
	        # Hidden column can be vary from each file, just inherit GC_loader_split class and overwrite isHiddenCol function
	        if col.find('_intervalid')>=0 :
	            self.pandasData[col] = np.arange(start=self.seq, stop=self.seq + self.numRawRecords, step=1)
	        elif col == 'projectid':
	            self.pandasData[col] = np.array([self.projectid] * self.numRawRecords)
	        elif col.find('order') >= 0:
	            self.pandasData[col] = np.array([order + 1] * self.numRawRecords)
	        elif self.table[0].find('collar')>=0 and col.find('dhid')>=0:
	            self.pandasData[col] = np.arange(start=self.seq, stop=self.seq + self.numRawRecords, step=1)
	        elif self.table[0].find('collar')>=0 and col.find('point')>=0:
	            self.pandasData[col] = np.array(['01010000A0417D0000000000000000000000000000000000000000000000000000'] * self.numRawRecords)
	
	
	        elif self.table[0].find('survey')>=0 and col.find('x')>=0:
	            self.pandasData[col] = np.array([0.0] * self.numRawRecords)
	        elif self.table[0].find('survey')>=0 and col.find('y')>=0:
	            self.pandasData[col] = np.array([0.0] * self.numRawRecords)
	        elif self.table[0].find('survey')>=0 and col.find('z')>=0:
d94fea5ae536a4216fb9410d06f535fd|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|507|1|V|python
	    def genHiddenCol(self,col,order):
	        self.fileCols.append(col)
	        # Hidden column can be vary from each file, just inherit GC_loader_split class and overwrite isHiddenCol function
	        if col.find('_intervalid')>=0 :
	            self.pandasData[col] = np.arange(start=self.seq, stop=self.seq + self.numRawRecords, step=1)
	        elif col == 'projectid':
	            self.pandasData[col] = np.array([self.projectid] * self.numRawRecords)
	        elif col.find('order') >= 0:
	            self.pandasData[col] = np.array([order + 1] * self.numRawRecords)
	        elif self.table[0].find('collar')>=0 and col.find('dhid')>=0:
	            self.pandasData[col] = np.arange(start=self.seq, stop=self.seq + self.numRawRecords, step=1)
	        elif self.table[0].find('collar')>=0 and col.find('point')>=0:
	            self.pandasData[col] = np.array(['01010000A0417D0000000000000000000000000000000000000000000000000000'] * self.numRawRecords)
	
	
	        elif self.table[0].find('survey')>=0 and col.find('x')>=0:
	            self.pandasData[col] = np.array([0.0] * self.numRawRecords)
	        elif self.table[0].find('survey')>=0 and col.find('y')>=0:
	            self.pandasData[col] = np.array([0.0] * self.numRawRecords)
	        elif self.table[0].find('survey')>=0 and col.find('z')>=0:
	            self.pandasData[col] = np.array([0.0] * self.numRawRecords)
078d64d279987e42e1566b5615d34d33|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|507|1|V|python
	    def genHiddenCol(self,col,order):
	        self.fileCols.append(col)
	        # Hidden column can be vary from each file, just inherit GC_loader_split class and overwrite isHiddenCol function
	        if col.find('_intervalid')>=0 :
	            self.pandasData[col] = np.arange(start=self.seq, stop=self.seq + self.numRawRecords, step=1)
	        elif col == 'projectid':
	            self.pandasData[col] = np.array([self.projectid] * self.numRawRecords)
	        elif col.find('order') >= 0:
	            self.pandasData[col] = np.array([order + 1] * self.numRawRecords)
	        elif self.table[0].find('collar')>=0 and col.find('dhid')>=0:
	            self.pandasData[col] = np.arange(start=self.seq, stop=self.seq + self.numRawRecords, step=1)
	        elif self.table[0].find('collar')>=0 and col.find('point')>=0:
	            self.pandasData[col] = np.array(['01010000A0417D0000000000000000000000000000000000000000000000000000'] * self.numRawRecords)
	
	
	        elif self.table[0].find('survey')>=0 and col.find('x')>=0:
	            self.pandasData[col] = np.array([0.0] * self.numRawRecords)
	        elif self.table[0].find('survey')>=0 and col.find('y')>=0:
	            self.pandasData[col] = np.array([0.0] * self.numRawRecords)
	        elif self.table[0].find('survey')>=0 and col.find('z')>=0:
	            self.pandasData[col] = np.array([0.0] * self.numRawRecords)
	        elif self.table[0].find('survey')>=0 and col.find('survey_order')>=0:
43d6754d8cfaa634ff6c2bec91ace2a0|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|507|1|V|python
	    def genHiddenCol(self,col,order):
	        self.fileCols.append(col)
	        # Hidden column can be vary from each file, just inherit GC_loader_split class and overwrite isHiddenCol function
	        if col.find('_intervalid')>=0 :
	            self.pandasData[col] = np.arange(start=self.seq, stop=self.seq + self.numRawRecords, step=1)
	        elif col == 'projectid':
	            self.pandasData[col] = np.array([self.projectid] * self.numRawRecords)
	        elif col.find('order') >= 0:
	            self.pandasData[col] = np.array([order + 1] * self.numRawRecords)
	        elif self.table[0].find('collar')>=0 and col.find('dhid')>=0:
	            self.pandasData[col] = np.arange(start=self.seq, stop=self.seq + self.numRawRecords, step=1)
	        elif self.table[0].find('collar')>=0 and col.find('point')>=0:
	            self.pandasData[col] = np.array(['01010000A0417D0000000000000000000000000000000000000000000000000000'] * self.numRawRecords)
	
	
	        elif self.table[0].find('survey')>=0 and col.find('x')>=0:
	            self.pandasData[col] = np.array([0.0] * self.numRawRecords)
	        elif self.table[0].find('survey')>=0 and col.find('y')>=0:
	            self.pandasData[col] = np.array([0.0] * self.numRawRecords)
	        elif self.table[0].find('survey')>=0 and col.find('z')>=0:
	            self.pandasData[col] = np.array([0.0] * self.numRawRecords)
	        elif self.table[0].find('survey')>=0 and col.find('survey_order')>=0:
	            self.pandasData[col] = np.array([0] * self.numRawRecords)
238850aa288ba04699fc7d98281d9dd0|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|507|1|V|python
	    def genHiddenCol(self,col,order):
	        self.fileCols.append(col)
	        # Hidden column can be vary from each file, just inherit GC_loader_split class and overwrite isHiddenCol function
	        if col.find('_intervalid')>=0 :
	            self.pandasData[col] = np.arange(start=self.seq, stop=self.seq + self.numRawRecords, step=1)
	        elif col == 'projectid':
	            self.pandasData[col] = np.array([self.projectid] * self.numRawRecords)
	        elif col.find('order') >= 0:
	            self.pandasData[col] = np.array([order + 1] * self.numRawRecords)
	        elif self.table[0].find('collar')>=0 and col.find('dhid')>=0:
	            self.pandasData[col] = np.arange(start=self.seq, stop=self.seq + self.numRawRecords, step=1)
	        elif self.table[0].find('collar')>=0 and col.find('point')>=0:
	            self.pandasData[col] = np.array(['01010000A0417D0000000000000000000000000000000000000000000000000000'] * self.numRawRecords)
	
	
	        elif self.table[0].find('survey')>=0 and col.find('x')>=0:
	            self.pandasData[col] = np.array([0.0] * self.numRawRecords)
	        elif self.table[0].find('survey')>=0 and col.find('y')>=0:
	            self.pandasData[col] = np.array([0.0] * self.numRawRecords)
	        elif self.table[0].find('survey')>=0 and col.find('z')>=0:
	            self.pandasData[col] = np.array([0.0] * self.numRawRecords)
	        elif self.table[0].find('survey')>=0 and col.find('survey_order')>=0:
	            self.pandasData[col] = np.array([0] * self.numRawRecords)
	        elif self.table[0].find('survey')>=0 and col.find('point')>=0:
a8a44a25a5b182c00765bc8bd88825d9|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|507|1|V|python
	    def genHiddenCol(self,col,order):
	        self.fileCols.append(col)
	        # Hidden column can be vary from each file, just inherit GC_loader_split class and overwrite isHiddenCol function
	        if col.find('_intervalid')>=0 :
	            self.pandasData[col] = np.arange(start=self.seq, stop=self.seq + self.numRawRecords, step=1)
	        elif col == 'projectid':
	            self.pandasData[col] = np.array([self.projectid] * self.numRawRecords)
	        elif col.find('order') >= 0:
	            self.pandasData[col] = np.array([order + 1] * self.numRawRecords)
	        elif self.table[0].find('collar')>=0 and col.find('dhid')>=0:
	            self.pandasData[col] = np.arange(start=self.seq, stop=self.seq + self.numRawRecords, step=1)
	        elif self.table[0].find('collar')>=0 and col.find('point')>=0:
	            self.pandasData[col] = np.array(['01010000A0417D0000000000000000000000000000000000000000000000000000'] * self.numRawRecords)
	
	
	        elif self.table[0].find('survey')>=0 and col.find('x')>=0:
	            self.pandasData[col] = np.array([0.0] * self.numRawRecords)
	        elif self.table[0].find('survey')>=0 and col.find('y')>=0:
	            self.pandasData[col] = np.array([0.0] * self.numRawRecords)
	        elif self.table[0].find('survey')>=0 and col.find('z')>=0:
	            self.pandasData[col] = np.array([0.0] * self.numRawRecords)
	        elif self.table[0].find('survey')>=0 and col.find('survey_order')>=0:
	            self.pandasData[col] = np.array([0] * self.numRawRecords)
	        elif self.table[0].find('survey')>=0 and col.find('point')>=0:
	            self.pandasData[col] = np.array(['01010000A0417D0000000000000000000000000000000000000000000000000000'] * self.numRawRecords)
d85b82668fcedd623b42ebcb3a6f3d61|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|507|1|V|python
	    def genHiddenCol(self,col,order):
	        self.fileCols.append(col)
	        # Hidden column can be vary from each file, just inherit GC_loader_split class and overwrite isHiddenCol function
	        if col.find('_intervalid')>=0 :
	            self.pandasData[col] = np.arange(start=self.seq, stop=self.seq + self.numRawRecords, step=1)
	        elif col == 'projectid':
	            self.pandasData[col] = np.array([self.projectid] * self.numRawRecords)
	        elif col.find('order') >= 0:
	            self.pandasData[col] = np.array([order + 1] * self.numRawRecords)
	        elif self.table[0].find('collar')>=0 and col.find('dhid')>=0:
	            self.pandasData[col] = np.arange(start=self.seq, stop=self.seq + self.numRawRecords, step=1)
	        elif self.table[0].find('collar')>=0 and col.find('point')>=0:
	            self.pandasData[col] = np.array(['01010000A0417D0000000000000000000000000000000000000000000000000000'] * self.numRawRecords)
	
	
	        elif self.table[0].find('survey')>=0 and col.find('x')>=0:
	            self.pandasData[col] = np.array([0.0] * self.numRawRecords)
	        elif self.table[0].find('survey')>=0 and col.find('y')>=0:
	            self.pandasData[col] = np.array([0.0] * self.numRawRecords)
	        elif self.table[0].find('survey')>=0 and col.find('z')>=0:
	            self.pandasData[col] = np.array([0.0] * self.numRawRecords)
	        elif self.table[0].find('survey')>=0 and col.find('survey_order')>=0:
	            self.pandasData[col] = np.array([0] * self.numRawRecords)
	        elif self.table[0].find('survey')>=0 and col.find('point')>=0:
	            self.pandasData[col] = np.array(['01010000A0417D0000000000000000000000000000000000000000000000000000'] * self.numRawRecords)
	        #  elif col=='chemical_element':
0e3bae45d99996ac58bba00637ec6bac|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|507|1|V|python
	    def genHiddenCol(self,col,order):
	        self.fileCols.append(col)
	        # Hidden column can be vary from each file, just inherit GC_loader_split class and overwrite isHiddenCol function
	        if col.find('_intervalid')>=0 :
	            self.pandasData[col] = np.arange(start=self.seq, stop=self.seq + self.numRawRecords, step=1)
	        elif col == 'projectid':
	            self.pandasData[col] = np.array([self.projectid] * self.numRawRecords)
	        elif col.find('order') >= 0:
	            self.pandasData[col] = np.array([order + 1] * self.numRawRecords)
	        elif self.table[0].find('collar')>=0 and col.find('dhid')>=0:
	            self.pandasData[col] = np.arange(start=self.seq, stop=self.seq + self.numRawRecords, step=1)
	        elif self.table[0].find('collar')>=0 and col.find('point')>=0:
	            self.pandasData[col] = np.array(['01010000A0417D0000000000000000000000000000000000000000000000000000'] * self.numRawRecords)
	
	
	        elif self.table[0].find('survey')>=0 and col.find('x')>=0:
	            self.pandasData[col] = np.array([0.0] * self.numRawRecords)
	        elif self.table[0].find('survey')>=0 and col.find('y')>=0:
	            self.pandasData[col] = np.array([0.0] * self.numRawRecords)
	        elif self.table[0].find('survey')>=0 and col.find('z')>=0:
	            self.pandasData[col] = np.array([0.0] * self.numRawRecords)
	        elif self.table[0].find('survey')>=0 and col.find('survey_order')>=0:
	            self.pandasData[col] = np.array([0] * self.numRawRecords)
	        elif self.table[0].find('survey')>=0 and col.find('point')>=0:
	            self.pandasData[col] = np.array(['01010000A0417D0000000000000000000000000000000000000000000000000000'] * self.numRawRecords)
	        #  elif col=='chemical_element':
	            #  self.pandasData[col] = np.array(['Au'] * self.numRawRecords)
07060c171702961a14f2f66a655373e7|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|507|1|V|python
	    def genHiddenCol(self,col,order):
	        self.fileCols.append(col)
	        # Hidden column can be vary from each file, just inherit GC_loader_split class and overwrite isHiddenCol function
	        if col.find('_intervalid')>=0 :
	            self.pandasData[col] = np.arange(start=self.seq, stop=self.seq + self.numRawRecords, step=1)
	        elif col == 'projectid':
	            self.pandasData[col] = np.array([self.projectid] * self.numRawRecords)
	        elif col.find('order') >= 0:
	            self.pandasData[col] = np.array([order + 1] * self.numRawRecords)
	        elif self.table[0].find('collar')>=0 and col.find('dhid')>=0:
	            self.pandasData[col] = np.arange(start=self.seq, stop=self.seq + self.numRawRecords, step=1)
	        elif self.table[0].find('collar')>=0 and col.find('point')>=0:
	            self.pandasData[col] = np.array(['01010000A0417D0000000000000000000000000000000000000000000000000000'] * self.numRawRecords)
	
	
	        elif self.table[0].find('survey')>=0 and col.find('x')>=0:
	            self.pandasData[col] = np.array([0.0] * self.numRawRecords)
	        elif self.table[0].find('survey')>=0 and col.find('y')>=0:
	            self.pandasData[col] = np.array([0.0] * self.numRawRecords)
	        elif self.table[0].find('survey')>=0 and col.find('z')>=0:
	            self.pandasData[col] = np.array([0.0] * self.numRawRecords)
	        elif self.table[0].find('survey')>=0 and col.find('survey_order')>=0:
	            self.pandasData[col] = np.array([0] * self.numRawRecords)
	        elif self.table[0].find('survey')>=0 and col.find('point')>=0:
	            self.pandasData[col] = np.array(['01010000A0417D0000000000000000000000000000000000000000000000000000'] * self.numRawRecords)
	        #  elif col=='chemical_element':
	            #  self.pandasData[col] = np.array(['Au'] * self.numRawRecords)
	        #  elif col=='measurement_unit':
9ab3e67e24df1387b0f2632cd4c239a8|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|507|1|V|python
	    def genHiddenCol(self,col,order):
	        self.fileCols.append(col)
	        # Hidden column can be vary from each file, just inherit GC_loader_split class and overwrite isHiddenCol function
	        if col.find('_intervalid')>=0 :
	            self.pandasData[col] = np.arange(start=self.seq, stop=self.seq + self.numRawRecords, step=1)
	        elif col == 'projectid':
	            self.pandasData[col] = np.array([self.projectid] * self.numRawRecords)
	        elif col.find('order') >= 0:
	            self.pandasData[col] = np.array([order + 1] * self.numRawRecords)
	        elif self.table[0].find('collar')>=0 and col.find('dhid')>=0:
	            self.pandasData[col] = np.arange(start=self.seq, stop=self.seq + self.numRawRecords, step=1)
	        elif self.table[0].find('collar')>=0 and col.find('point')>=0:
	            self.pandasData[col] = np.array(['01010000A0417D0000000000000000000000000000000000000000000000000000'] * self.numRawRecords)
	
	
	        elif self.table[0].find('survey')>=0 and col.find('x')>=0:
	            self.pandasData[col] = np.array([0.0] * self.numRawRecords)
	        elif self.table[0].find('survey')>=0 and col.find('y')>=0:
	            self.pandasData[col] = np.array([0.0] * self.numRawRecords)
	        elif self.table[0].find('survey')>=0 and col.find('z')>=0:
	            self.pandasData[col] = np.array([0.0] * self.numRawRecords)
	        elif self.table[0].find('survey')>=0 and col.find('survey_order')>=0:
	            self.pandasData[col] = np.array([0] * self.numRawRecords)
	        elif self.table[0].find('survey')>=0 and col.find('point')>=0:
	            self.pandasData[col] = np.array(['01010000A0417D0000000000000000000000000000000000000000000000000000'] * self.numRawRecords)
	        #  elif col=='chemical_element':
	            #  self.pandasData[col] = np.array(['Au'] * self.numRawRecords)
	        #  elif col=='measurement_unit':
	            #  self.pandasData[col] = np.array(['opt'] * self.numRawRecords)
8b41866a41c92c4f8a218bc697f2e11a|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|507|1|V|python
	    def genHiddenCol(self,col,order):
	        self.fileCols.append(col)
	        # Hidden column can be vary from each file, just inherit GC_loader_split class and overwrite isHiddenCol function
	        if col.find('_intervalid')>=0 :
	            self.pandasData[col] = np.arange(start=self.seq, stop=self.seq + self.numRawRecords, step=1)
	        elif col == 'projectid':
	            self.pandasData[col] = np.array([self.projectid] * self.numRawRecords)
	        elif col.find('order') >= 0:
	            self.pandasData[col] = np.array([order + 1] * self.numRawRecords)
	        elif self.table[0].find('collar')>=0 and col.find('dhid')>=0:
	            self.pandasData[col] = np.arange(start=self.seq, stop=self.seq + self.numRawRecords, step=1)
	        elif self.table[0].find('collar')>=0 and col.find('point')>=0:
	            self.pandasData[col] = np.array(['01010000A0417D0000000000000000000000000000000000000000000000000000'] * self.numRawRecords)
	
	
	        elif self.table[0].find('survey')>=0 and col.find('x')>=0:
	            self.pandasData[col] = np.array([0.0] * self.numRawRecords)
	        elif self.table[0].find('survey')>=0 and col.find('y')>=0:
	            self.pandasData[col] = np.array([0.0] * self.numRawRecords)
	        elif self.table[0].find('survey')>=0 and col.find('z')>=0:
	            self.pandasData[col] = np.array([0.0] * self.numRawRecords)
	        elif self.table[0].find('survey')>=0 and col.find('survey_order')>=0:
	            self.pandasData[col] = np.array([0] * self.numRawRecords)
	        elif self.table[0].find('survey')>=0 and col.find('point')>=0:
	            self.pandasData[col] = np.array(['01010000A0417D0000000000000000000000000000000000000000000000000000'] * self.numRawRecords)
	        #  elif col=='chemical_element':
	            #  self.pandasData[col] = np.array(['Au'] * self.numRawRecords)
	        #  elif col=='measurement_unit':
	            #  self.pandasData[col] = np.array(['opt'] * self.numRawRecords)
	        elif col=='ingestiontime':
0f68368dd629880eaf3efeb0fc8e85da|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|507|1|V|python
	    def genHiddenCol(self,col,order):
	        self.fileCols.append(col)
	        # Hidden column can be vary from each file, just inherit GC_loader_split class and overwrite isHiddenCol function
	        if col.find('_intervalid')>=0 :
	            self.pandasData[col] = np.arange(start=self.seq, stop=self.seq + self.numRawRecords, step=1)
	        elif col == 'projectid':
	            self.pandasData[col] = np.array([self.projectid] * self.numRawRecords)
	        elif col.find('order') >= 0:
	            self.pandasData[col] = np.array([order + 1] * self.numRawRecords)
	        elif self.table[0].find('collar')>=0 and col.find('dhid')>=0:
	            self.pandasData[col] = np.arange(start=self.seq, stop=self.seq + self.numRawRecords, step=1)
	        elif self.table[0].find('collar')>=0 and col.find('point')>=0:
	            self.pandasData[col] = np.array(['01010000A0417D0000000000000000000000000000000000000000000000000000'] * self.numRawRecords)
	
	
	        elif self.table[0].find('survey')>=0 and col.find('x')>=0:
	            self.pandasData[col] = np.array([0.0] * self.numRawRecords)
	        elif self.table[0].find('survey')>=0 and col.find('y')>=0:
	            self.pandasData[col] = np.array([0.0] * self.numRawRecords)
	        elif self.table[0].find('survey')>=0 and col.find('z')>=0:
	            self.pandasData[col] = np.array([0.0] * self.numRawRecords)
	        elif self.table[0].find('survey')>=0 and col.find('survey_order')>=0:
	            self.pandasData[col] = np.array([0] * self.numRawRecords)
	        elif self.table[0].find('survey')>=0 and col.find('point')>=0:
	            self.pandasData[col] = np.array(['01010000A0417D0000000000000000000000000000000000000000000000000000'] * self.numRawRecords)
	        #  elif col=='chemical_element':
	            #  self.pandasData[col] = np.array(['Au'] * self.numRawRecords)
	        #  elif col=='measurement_unit':
	            #  self.pandasData[col] = np.array(['opt'] * self.numRawRecords)
	        elif col=='ingestiontime':
	            self.pandasData[col] = np.array([str(datetime.datetime.now())] * self.numRawRecords)
affde7f20092f17d19a711427d786fa8|file:///Users/jerry/repo/ibmeww-etl/script/common/base_loader.py|509|1|V|python
	        Keyword arguments:
	            table_data -- the data your want to insert
	            sql_header - the header part of the sql
d8b39d156ee251975616820b75c7e3b7|file:///Users/jerry/repo/ibmeww-etl/script/surface/surface.py|59|9|v|python
	seq_number 
6d774aa355b3d6e93ee5b6b9cff7d096|file:///Users/jerry/repo/ibmeww-etl/script/common/base_loader.py|372|1|V|python
	        Keyword arguments:
	            panda_table_data -- the pandas data frame you want to filter null
caf1908d07e4d48dc592abfd5782fe8b|file:///Users/jerry/repo/ibmeww-etl/script/common/base_loader.py|586|1|V|python
	        Keyword arguments:
	            col -- the column name
	            order - the order, usually 0 - index, 1 - data
	            seq_number - the sequence start number for index tables
	            project_id - the project id
	            index_table_name - the main/index table name
7a6ab3bb4317711b0160127e0354c183|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|459|1|V|python
	    def unify_datetype_ymd(self,tableData,date_column):
	        tableData[date_column]=tableData[date_column].fillna('01-01-01 00:00:00')
	        tableData[date_column]=pd.to_datetime(tableData[date_column])
	        tableData[date_column]=tableData[date_column].astype(str)
5637dd7bb19926258bd812d7c61d425a|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|534|1|V|python
	                        if date_column in list(self.pandasData.columns.values):
fbcac8471e0820b25d4a04e4b2a76414|file:///Users/jerry/repo/ibmeww-etl/script/common/base_loader.py|618|1|V|python
	        elif index_table_name.find('collar')>=0 and col.find('point')>=0:
	            self.pandas_data[col] = np.array(['01010000A0417D0000000000000000000000000000000000000000000000000000'] * numRawRecords)
d74a46b1421c05230ad8da15e4efae72|file:///Users/jerry/repo/ibmeww-etl/script/common/base_loader.py|582|1|V|python
	                        fileCols.append(fileCol)
a6376f2ff345982f9307e2305b56795b|file:///Users/jerry/repo/ibmeww-etl/script/common/base_loader.py|571|1|V|python
	                fileCols.append(col)
45f2897afacf319f07b302fa875910cd|file:///Users/jerry/repo/ibmeww-etl/script/common/base_loader.py|593|9|v|python
	add_missing_columns
6b0ea44011f0f9a408d68779389f9446|file:///Users/jerry/repo/ibmeww-etl/script/common/base_loader.py|594|1|V|python
	    def add_missing_columns(self, table_name, table_data):
f538fa954941cabe5763c9c6b8b2124c|file:///Users/jerry/repo/ibmeww-etl/script/surface/surface.py|46|1|V|python
	                table_data['chemical_element'] = 'Au'
fc37a331971de9b4fbb10e0a4809a446|file:///Users/jerry/repo/ibmeww-etl/script/common/base_loader.py|637|1|V|python
	        elif index_table_name.find('grab')>=0 and col.find('point')>=0:
	            self.pandas_data[col] = np.array(['01010000A0417D0000000000000000000000000000000000000000000000000000'] * numRawRecords)
d4eaa365c8ff727a2a8d6f7e6d3ab231|file:///Users/jerry/repo/ibmeww-etl/script/surface/surface.py|1|1|V|python
	        project_code = self.get_project_code_from_file_name()
c5ace6a4664ab2a62fc9e9464196a8cf|file:///Users/jerry/repo/ibmeww-etl/script/surface/surface.py|1|1|V|python
	        project_code = self.get_project_code_from_file_name()
	        print('project code is {}'.format(project_code))
774f9a66dd82857f53674df475ccebda|file:///Users/jerry/repo/ibmeww-etl/script/surface/surface.py|1|1|V|python
	        project_code = self.get_project_code_from_file_name()
	        print('project code is {}'.format(project_code))
	        print('0.3 get the project id')
2913b213786edabe41484dc2b556dd4b|file:///Users/jerry/repo/ibmeww-etl/script/surface/surface.py|1|1|V|python
	        project_code = self.get_project_code_from_file_name()
	        print('project code is {}'.format(project_code))
	        print('0.3 get the project id')
	        project_id = self.get_project_id_by_code(project_code)
75e4c3cb5820fa1a99d49fb82a326a16|file:///Users/jerry/repo/ibmeww-etl/script/surface/surface.py|1|1|V|python
	        project_code = self.get_project_code_from_file_name()
	        print('project code is {}'.format(project_code))
	        print('0.3 get the project id')
	        project_id = self.get_project_id_by_code(project_code)
	        print('project id is {}'.format(project_id))
c9b1909366281deac9faa84763362704|file:///Users/jerry/repo/ibmeww-etl/script/surface/surface.py|1|1|V|python
	        project_code = self.get_project_code_from_file_name()
	        print('project code is {}'.format(project_code))
	        print('0.3 get the project id')
	        project_id = self.get_project_id_by_code(project_code)
	        print('project id is {}'.format(project_id))
	        print('0.4 get the table name to be used to load the data')
e1794cd7073b238c046a68b981db07a2|file:///Users/jerry/repo/ibmeww-etl/script/surface/surface.py|1|1|V|python
	        project_code = self.get_project_code_from_file_name()
	        print('project code is {}'.format(project_code))
	        print('0.3 get the project id')
	        project_id = self.get_project_id_by_code(project_code)
	        print('project id is {}'.format(project_id))
	        print('0.4 get the table name to be used to load the data')
	        table_names = self.get_table_names()
67d8e9e55ab590fcef165b9a77e40acb|file:///Users/jerry/repo/ibmeww-etl/script/surface/surface.py|1|1|V|python
	        project_code = self.get_project_code_from_file_name()
	        print('project code is {}'.format(project_code))
	        print('0.3 get the project id')
	        project_id = self.get_project_id_by_code(project_code)
	        print('project id is {}'.format(project_id))
	        print('0.4 get the table name to be used to load the data')
	        table_names = self.get_table_names()
	        print('the table names are {}'.format(table_names))
22e14005aeecc0d49f722d60c4dc57e1|file:///Users/jerry/repo/ibmeww-etl/script/surface/surface.py|1|1|V|python
	        project_code = self.get_project_code_from_file_name()
	        print('project code is {}'.format(project_code))
	        print('0.3 get the project id')
	        project_id = self.get_project_id_by_code(project_code)
	        print('project id is {}'.format(project_id))
	        print('0.4 get the table name to be used to load the data')
	        table_names = self.get_table_names()
	        print('the table names are {}'.format(table_names))
	        print('0.5 get the table columns')
42b31d7342a05e3e7bc5f265447c4caf|file:///Users/jerry/repo/ibmeww-etl/script/surface/surface.py|1|1|V|python
	        project_code = self.get_project_code_from_file_name()
	        print('project code is {}'.format(project_code))
	        print('0.3 get the project id')
	        project_id = self.get_project_id_by_code(project_code)
	        print('project id is {}'.format(project_id))
	        print('0.4 get the table name to be used to load the data')
	        table_names = self.get_table_names()
	        print('the table names are {}'.format(table_names))
	        print('0.5 get the table columns')
	        table_columns = self.get_table_columns(self.temp_schema, table_names)
137bd574caf3522a5e26362d461be252|file:///Users/jerry/repo/ibmeww-etl/script/surface/surface.py|1|1|V|python
	        project_code = self.get_project_code_from_file_name()
	        print('project code is {}'.format(project_code))
	        print('0.3 get the project id')
	        project_id = self.get_project_id_by_code(project_code)
	        print('project id is {}'.format(project_id))
	        print('0.4 get the table name to be used to load the data')
	        table_names = self.get_table_names()
	        print('the table names are {}'.format(table_names))
	        print('0.5 get the table columns')
	        table_columns = self.get_table_columns(self.temp_schema, table_names)
	        print('the table columns are {}'.format(table_columns))
ccd4fa9e23cc2a76c34d7a44e21ea805|file:///Users/jerry/repo/ibmeww-etl/script/surface/surface.py|1|1|V|python
	        project_code = self.get_project_code_from_file_name()
	        print('project code is {}'.format(project_code))
	        print('0.3 get the project id')
	        project_id = self.get_project_id_by_code(project_code)
	        print('project id is {}'.format(project_id))
	        print('0.4 get the table name to be used to load the data')
	        table_names = self.get_table_names()
	        print('the table names are {}'.format(table_names))
	        print('0.5 get the table columns')
	        table_columns = self.get_table_columns(self.temp_schema, table_names)
	        print('the table columns are {}'.format(table_columns))
	        print('0.6 get the column types')
3ebd5cbcc2493d7c8c5e00d64c15241e|file:///Users/jerry/repo/ibmeww-etl/script/surface/surface.py|1|1|V|python
	        project_code = self.get_project_code_from_file_name()
	        print('project code is {}'.format(project_code))
	        print('0.3 get the project id')
	        project_id = self.get_project_id_by_code(project_code)
	        print('project id is {}'.format(project_id))
	        print('0.4 get the table name to be used to load the data')
	        table_names = self.get_table_names()
	        print('the table names are {}'.format(table_names))
	        print('0.5 get the table columns')
	        table_columns = self.get_table_columns(self.temp_schema, table_names)
	        print('the table columns are {}'.format(table_columns))
	        print('0.6 get the column types')
	        #  column_types = self.get_column_type(table_names, table_columns)
1f27b1ef478b2f4ee4b612b44151445a|file:///Users/jerry/repo/ibmeww-etl/script/surface/surface.py|1|1|V|python
	        project_code = self.get_project_code_from_file_name()
	        print('project code is {}'.format(project_code))
	        print('0.3 get the project id')
	        project_id = self.get_project_id_by_code(project_code)
	        print('project id is {}'.format(project_id))
	        print('0.4 get the table name to be used to load the data')
	        table_names = self.get_table_names()
	        print('the table names are {}'.format(table_names))
	        print('0.5 get the table columns')
	        table_columns = self.get_table_columns(self.temp_schema, table_names)
	        print('the table columns are {}'.format(table_columns))
	        print('0.6 get the column types')
	        #  column_types = self.get_column_type(table_names, table_columns)
	        #  print('column types are {}'.format(column_types))
5f2117a77347a606d419f57050f27fe5|file:///Users/jerry/repo/ibmeww-etl/script/surface/surface.py|1|1|V|python
	        project_code = self.get_project_code_from_file_name()
	        print('project code is {}'.format(project_code))
	        print('0.3 get the project id')
	        project_id = self.get_project_id_by_code(project_code)
	        print('project id is {}'.format(project_id))
	        print('0.4 get the table name to be used to load the data')
	        table_names = self.get_table_names()
	        print('the table names are {}'.format(table_names))
	        print('0.5 get the table columns')
	        table_columns = self.get_table_columns(self.temp_schema, table_names)
	        print('the table columns are {}'.format(table_columns))
	        print('0.6 get the column types')
	        #  column_types = self.get_column_type(table_names, table_columns)
	        #  print('column types are {}'.format(column_types))
	        num_split = self.get_split_num(table_names, table_columns, self.dic_match)
0ecbe24e8fb18a58ef055d4a3c1e8693|file:///Users/jerry/repo/ibmeww-etl/script/surface/surface.py|1|1|V|python
	        project_code = self.get_project_code_from_file_name()
	        print('project code is {}'.format(project_code))
	        print('0.3 get the project id')
	        project_id = self.get_project_id_by_code(project_code)
	        print('project id is {}'.format(project_id))
	        print('0.4 get the table name to be used to load the data')
	        table_names = self.get_table_names()
	        print('the table names are {}'.format(table_names))
	        print('0.5 get the table columns')
	        table_columns = self.get_table_columns(self.temp_schema, table_names)
	        print('the table columns are {}'.format(table_columns))
	        print('0.6 get the column types')
	        #  column_types = self.get_column_type(table_names, table_columns)
	        #  print('column types are {}'.format(column_types))
	        num_split = self.get_split_num(table_names, table_columns, self.dic_match)
	        print('0.7 get the seq number')
204346fa30dede02aecab27e77624f7f|file:///Users/jerry/repo/ibmeww-etl/script/surface/surface.py|1|1|V|python
	        project_code = self.get_project_code_from_file_name()
	        print('project code is {}'.format(project_code))
	        print('0.3 get the project id')
	        project_id = self.get_project_id_by_code(project_code)
	        print('project id is {}'.format(project_id))
	        print('0.4 get the table name to be used to load the data')
	        table_names = self.get_table_names()
	        print('the table names are {}'.format(table_names))
	        print('0.5 get the table columns')
	        table_columns = self.get_table_columns(self.temp_schema, table_names)
	        print('the table columns are {}'.format(table_columns))
	        print('0.6 get the column types')
	        #  column_types = self.get_column_type(table_names, table_columns)
	        #  print('column types are {}'.format(column_types))
	        num_split = self.get_split_num(table_names, table_columns, self.dic_match)
	        print('0.7 get the seq number')
	        seq_number = self.get_seq_value(table_names[0])
d0c3813a3202a29ef25091109b345e9f|file:///Users/jerry/repo/ibmeww-etl/script/surface/surface.py|1|1|V|python
	        project_code = self.get_project_code_from_file_name()
	        print('project code is {}'.format(project_code))
	        print('0.3 get the project id')
	        project_id = self.get_project_id_by_code(project_code)
	        print('project id is {}'.format(project_id))
	        print('0.4 get the table name to be used to load the data')
	        table_names = self.get_table_names()
	        print('the table names are {}'.format(table_names))
	        print('0.5 get the table columns')
	        table_columns = self.get_table_columns(self.temp_schema, table_names)
	        print('the table columns are {}'.format(table_columns))
	        print('0.6 get the column types')
	        #  column_types = self.get_column_type(table_names, table_columns)
	        #  print('column types are {}'.format(column_types))
	        num_split = self.get_split_num(table_names, table_columns, self.dic_match)
	        print('0.7 get the seq number')
	        seq_number = self.get_seq_value(table_names[0])
	        print('seq numbers is {}'.format(seq_number))
c6e6ab5a914bb4181931c9f0181f7a14|file:///Users/jerry/repo/ibmeww-etl/script/common/base_loader.py|666|16|v|python
	table_names, table_columns, project_id, seq_number, num_split
66c3c8ec83b355669c2002fa75e8a637|file:///Users/jerry/repo/ibmeww-etl/script/common/base_loader.py|596|1|V|python
	        raise NotImplementedError
5713ae8343bac2ca9606f112654aef1b|file:///Users/jerry/repo/ibmeww-etl/script/common/base_loader.py|598|9|v|python
	pre_process_pandas_data
4cfe88338b458c723b755ba4e70171ac|file:///Users/jerry/repo/ibmeww-etl/script/common/base_loader.py|599|1|V|python
	        '''implement this method to process something before moving on
	
	        '''
3091c50f236ecc7f671d8df0e2eed6f6|file:///Users/jerry/repo/ibmeww-etl/script/common/base_loader.py|680|1|V|python
	    def step_0_tasks(self):
121bb24c6f4b708975098e4e4961b265|file:///Users/jerry/repo/ibmeww-etl/script/common/base_loader.py|684|1|V|python
	        print('0.1 do some check to make sure the load process could proceed')
c75a3593ee7347c52d505bc37bfdb308|file:///Users/jerry/repo/ibmeww-etl/script/common/base_loader.py|683|1|V|python
	    def load(self):
	        print('0.1 do some check to make sure the load process could proceed')
c0c1750162f69bb5b23f70373ade91d7|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|37|1|V|python
	logger=log.get_logger("FileStoreHandler")
8c0379351cf7b8ba1faab30b2dc4908f|file:///Users/jerry/repo/ibmeww-etl/script/common/base_loader.py|714|1|V|python
	        logger.info('\t1.5 check if the loaded data is empty or not')
43baa002529eb3e5d116d3f36e3a329f|file:///Users/jerry/repo/ibmeww-etl/script/common/base_loader.py|727|1|V|python
	            logger.info('2.2 the table name is {}'.format(table))
50f8d820bb8efd0f7e8dfcb9cce6547c|file:///Users/jerry/repo/ibmeww-etl/script/common/base_loader.py|704|1|V|python
	        logger.info('\t1.2 load the data in raw format, without using pandas')
598aa10de7f975f95dceed2a9cb7883c|file:///Users/jerry/repo/ibmeww-etl/script/common/base_loader.py|730|1|V|python
	            logger.info('-----------------------------------------------------')
3fdd79373c47f7dc6eb9243b97a5e96c|file:///Users/jerry/repo/ibmeww-etl/script/common/base_loader.py|606|1|V|python
	    def post_process_table_data(self):
d7fc0fadd3c02d99b36da77c6a7b338e|file:///Users/jerry/repo/ibmeww-etl/script/common/base_loader.py|600|1|V|python
	    def pre_process_pandas_data(self):
f0db0e59180c99e24a5c0625bb845055|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill_service_dal.py|480|1|V|python
	    def update_collar(self):
	        sql = 'update drill_temp.collar_tmp set point=ST_SETSRID(ST_makepoint(x,y,z),32065)'
	        try:
	            self.pgsql.query(sql, reply=False)
	            self.pgsql.commit()
	        except Exception as e:
	            traceback.print_exc()
	            self.pgsql.rollback()
	            raise gip_exception.GIPException(e)
6e10d4220bef8c632150856c4ffc7e7e|file:///Users/jerry/repo/ibmeww-etl/script/common/base_loader.py|617|1|V|python
	            self.pgsql.query(sql, reply=False)
f508ed6df7a3fbe8d08c52068ca798b7|file:///Users/jerry/repo/ibmeww-etl/script/common/pgsql_util.py|1|1|V|python
	#!/usr/bin/env python3
	# -*- coding: utf-8 -*-
b6bf6de79f70833d8a76154e142d34bb|file:///Users/jerry/repo/ibmeww-etl/script/common/pgsql_util.py|4|1|V|python
	
	def singleton(cls):
	    ''' singleton decorator
	
	    to make sure there is only one instance of the decorated class
	    '''
	    _instance = {}
	
	    def inner(*args, **kwargs):
	        if cls not in _instance:
	            _instance[cls] = cls(*args, **kwargs)
	        return _instance[cls]
	    return inner
c9c03892b6b719e6b79e8c520f067fe0|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill_service_dal.py|143|1|V|python
	        logger.info(sql)
366c0de52796267d50ea4b063713bd5a|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill.py|26|1|V|python
	import script.common.log_handler as log
a1f51ca827891dc77a3a30bef9f540ba|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill_service_dal.py|130|1|V|python
	    def add_fk(self,target_schema,target_table,target_column,refer_schema,refer_table,refer_column):
	        '''add foreign key constraint
	
	        Keyword arguments:
	            target_schema-- the real schema, not the tmp one
	            target_table -- the real table name
	            target_column -- the column name
	            refer_schema -- the schema which it refers to
	            refer_table -- the table name
	            refer_column -- the column name
	        '''
	        fk_name='rel'+'_'+target_table+'_'+refer_table
	        sql = 'ALTER TABLE {0}.{1} ADD CONSTRAINT {2} ' \
	              'FOREIGN KEY ({3}) REFERENCES {4}.{5}' \
	              ' ({6}) MATCH FULL ON DELETE NO ACTION ON UPDATE NO ACTION;'\
	            .format(target_schema,target_table,fk_name,target_column,refer_schema,refer_table,refer_column)
	        logger.info(sql)
	        self.pgsql.query(cmd=sql,reply=False)
4f9e7130509913ffd0166fefc953c991|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill_service_dal.py|263|1|V|python
	    def drop_fk(self,schema,table_tmp):
	        '''drop the foreign key in real table
	
	        Keyword arguments:
	            schema -- the real schema, not the tmp one
	            table_tmp -- the tmp table name
	
	        before moving data from tmp table to real table, we drop the foreign key
	        constraint in real table so the data can be moved from tmp tables to
	        real tables
	        '''
	        table_real = table_tmp.replace('_tmp', '')
	        fks = self.pgsql.queryForeignKey(schema=schema, name=table_real)
	        for fk in fks:
	            sql = 'alter table {0}.{1} drop constraint if exists {2} cascade;'.format(schema,table_real,fk)
	            self.pgsql.query(sql, reply=False)
	
d1e12cb0b2f679acd35b3ce3bcb10cf8|file:///Users/jerry/repo/ibmeww-etl/script/common/base_service.py|44|1|V|python
	        try:
	            self.pgsql.connect_uri(self.conf.get_dburi())
	            yield
	        except Exception as e:
	            traceback.print_exc()
	            self.pgsql.rollback()
	            raise gip_exception.GIPException(e)
	        finally:
	            self.pgsql.disconnect()
05342befa0556bd7f8de43d4916f1902|file:///Users/jerry/repo/ibmeww-etl/script/common/base_service.py|63|1|V|python
	        try:
	            yield
	        except Exception as e:
	            traceback.print_exc()
	            self.pgsql.rollback()
	            raise gip_exception.GIPException(e)
	        finally:
	            if commit:
	                self.pgsql.commit()
0e37d7b7cbc11565a7659d584593a106|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill_service_dal.py|62|1|V|python
	    def __copy_tmp2real(self,schema,table_tmp):
	        '''copy data from tmp table to real table
	
	        Keyword arguments:
	            schema -- the tmp schema, not the real one
	            table_tmp -- the tmp table, not real table
	        '''
	        table_real = table_tmp.replace('_tmp', '')
	        schema_real = schema.replace('_temp', '')
	        columns = self.pgsql.columns(table_real, schema_real)
	        columns = ','.join(columns)
	        sql = 'insert into {0}.{1}({2}) select {2} from {3}.{4}'.format(
	                schema_real,
	                table_real,
	                columns,
	                schema,
	                table_tmp)
	        print(sql)
	        self.pgsql.query(cmd=sql,reply=False)
a64367fc18057003a9d587347fafdad2|file:///Users/jerry/repo/ibmeww-etl/script/common/base_service.py|163|1|V|python
	        elif isinstance(table_real, collections.abc.Sequence) \
82f543897eeb63a155765dc0feb7352a|file:///Users/jerry/repo/ibmeww-etl/script/common/base_service.py|163|1|V|python
	        elif isinstance(table_real, collections.abc.Sequence) \
	                    and not isinstance(table_real, str):
2607a5bb8dba787e9ab646b0a66fb938|file:///Users/jerry/repo/ibmeww-etl/script/common/base_service.py|175|1|V|python
	    def pre_copy_tmp_to_real(self):
cadc6801650fbbe9232b0f0d48fbeae4|file:///Users/jerry/repo/ibmeww-etl/script/common/base_service.py|182|1|V|python
	    def post_copy_tmp_to_real(self):
f1a3a31c2bbdfcf1d7dbf57115080394|file:///Users/jerry/repo/ibmeww-etl/script/surface/surface.py|62|1|V|python
	            self.update_column_point(self.temp_schema, 'soilindex_tmp')
7ac04b43af5c26dd8db67c8f1a903e5c|file:///Users/jerry/repo/ibmeww-etl/script/surface/surface.py|61|1|V|python
	        if 'table_name' in kwargs and kwargs['table_name'] == 'soilindex_tmp':
	            self.update_column_point(self.temp_schema, 'grab_tmp')
	            self.update_column_point(self.temp_schema, 'soilindex_tmp')
3fb283105d062ef159bd02539e666fab|file:///Users/jerry/repo/ibmeww-etl/script/surface/surface.py|63|1|V|python
	        if 'table_name' in kwargs and kwargs['table_name'] == 'grab_tmp':
	            self.update_column_point(self.temp_schema, 'grab_tmp')
6d6ac48d49a5c53567cd185c5c81dc35|file:///Users/jerry/repo/ibmeww-etl/script/surface/surface.py|63|1|V|python
	        if 'table_name' in kwargs and kwargs['table_name'] == 'grab_tmp':
b4eff3639db19704613840731f02a95e|file:///Users/jerry/repo/ibmeww-etl/script/surface/surface.py|61|39|v|python
	kwargs['table_name'] == 'soilindex_tmp'
6bd6d9ceed95cb30e77ccfc0a11ff586|file:///Users/jerry/repo/ibmeww-etl/script/surface/surface.py|63|39|v|python
	kwargs['table_name'] == 'grab_tmp'
ab490eef35dde377de20f5704d16337f|file:///Users/jerry/repo/ibmeww-etl/script/surface/surface.py|64|1|V|python
	            self.update_column_point(self.temp_schema, 'grab_tmp')
28c4c0ffae49e09502fa40665f513822|file:///Users/jerry/repo/ibmeww-etl/script/drill/drill_service_dal.py|411|1|V|python
	    def materialized_view(self):
	        for table in DRILL_VIEW:
	            sql='REFRESH MATERIALIZED VIEW CONCURRENTLY drill.{0}'.format(table)
	            print(sql)
	            with self.__run_query_context():
	                self.pgsql.query(sql,reply=False)
dc3440f6281f96fbb5d3aa10cdc28e53|file:///Users/jerry/repo/ibmeww-etl/script/common/base_service.py|219|1|V|python
	        if isinstance(table_tmp, collections.abc.Sequence) \
	                    and not isinstance(table_tmp, str):
85a215cebedb6329e6581f9bc2b8c97d|file:///Users/jerry/repo/ibmeww-etl/script/surface/surface_service.py|110|1|V|python
	            logger.info('\t3. copy data from tmp table to real table')
ee657ea15fa017302b9cb1c97a67a909|file:///Users/jerry/repo/ibmeww-etl/script/surface/surface.py|62|13|V|python
	            self.tem
e6011b60a184ab4214e6a73d3cdd727a|file:///Users/jerry/dotfiles/nvim/.vim/config/dein.toml|11|1|V|toml
	[[plugins]]
	repo = 'honza/vim-snippets'
ced66a533cd1c2d5cc9939b356b365b5|file:///Users/jerry/dotfiles/nvim/.vim/config/dein.toml|16|31|v|toml
	honza/vim-snippets
6a21216de6daabddf52b8f41fdb62888|file:///Users/jerry/dotfiles/nvim/.vim/config/dein.toml|15|1|V|toml
	[[plugins]]
	repo = 'itchyny/vim-cursorword'
7bb0edd98f22430a03b67f853e83c2ca|file:///Users/jerry/repo/ibmeww-etl/script/surface/surface.py|60|1|V|python
	        
d964c8914ce378fe154ba259c4f5dd14|file:///Users/jerry/repo/ibmeww-etl/script/surface/surface.py|58|1|v|python
	 target_list
ff99bdf07c6f7e5f9be270997309b1cb|file:///Users/jerry/repo/ibmeww-etl/script/surface/surface.py|58|1|v|python
	target_list
ff0ec520769e6cb4d6a9061563dd5d2b|file:///Users/jerry/repo/ibmeww-etl/script/surface/surface.py|58|1|v|python
	for target_list
e1b6837c3afdea575cb7dbe2208b8dbe|file:///Users/jerry/repo/ibmeww-etl/script/surface/surface.py|45|2|V|python
	                print('there is Au_ppb column')
9987b371619e3810917be2af9411dc41|file:///Users/jerry/dotfiles/nvim/.vim/vimrc|49|18|v|vim
	jmcantrell/vim-virtualenv
5423db3bbc88d4894c99509340be9f05|file:///Users/jerry/.vim/config/lightline.vim|10|53|v|vim
	['tnt']
483f4e06a841ab442aeabcf753a718f2|file:///Users/jerry/repo/ibmeww-etl/script/blockModel/bm_service.py|417|158|v|python
	/Users/chen/Documents/GPS/customer/CPE/Goldcorp/data/sample/20180620/Datamine copy
70a98c547b0e3acdba651c76b103a7e5|file:///Users/jerry/dotfiles/nvim/.vim/config/dein.toml|59|1|V|toml
	[[plugins]]
	repo = 'kassio/neoterm'
	hook_add = 'source ~/.vim/config/terminal.vim'
28387d1c5eec13f5994c4ead34df6411|file:///Users/jerry/dotfiles/nvim/.vim/config/dein.toml|65|9|v|toml
	kassio/neoterm
bfab4e9eeab5d0865e41be118306f20b|file:///Users/jerry/dotfiles/nvim/.vim/config/dein.toml|66|13|v|toml
	source ~/.vim/config/terminal.vim
c21f969b5f03d33d43e04f8f136e7682|file:///Users/jerry/dotfiles/nvim/.vim/config/dein.toml|68|25|v|toml
	default
e47ee80712ff7aff69476479573de0c6|file:///Users/jerry/dotfiles/nvim/.vim/config/dein.toml|69|1|V|toml
	let g:keysound_py_version = 3
4c6492d751a2de20fde5527f482e5b2e|file:///Users/jerry/dotfiles/nvim/.vim/config/dein.toml|69|1|V|toml
	let g:keysound_volume = 500
6dca62f8d958c1c551bd8a6a5cee67bd|file:///Users/jerry/dotfiles/nvim/.vim/config/dein.toml|69|1|V|toml
	let g:keysound_py_version = 3
	let g:keysound_volume = 500
7c880aaec93d99a0db06184852b49e87|file:///Users/jerry/dotfiles/nvim/.vim/config/dein.toml|67|1|V|toml
	let g:keysound_enable = 1
9268e7aa81809a4289d8e039a153ad88|file:///Users/jerry/.vim/vimrc|185|7|v|vim
	python_host_prog  
866082a2adfc8cb64203728600790e58|file:///usr/local/Homebrew/Library/Taps/homebrew/homebrew-core/Formula/macvim.rb|50|27|V|ruby
	                          "--enable-luainterp",
e1a2ce9d5c6bfdb4f30d4e5f33d6adad|file:///Users/jerry/.vim/vimrc|33|18|v|vim
	justinmk/vim-sneak
1e0b1c2d8adb45a067490e8977de393d|file:///Users/jerry/.vim/vimrc|48|18|v|vim
	ConradIrwin/vim-bracketed-paste
068e570d6a32dcd2b36567ac21046d67|file:///Users/jerry/.vim/vimrc|57|18|v|vim
	lifepillar/vim-solarized8
4c6f72e3f6925afe4d21838343bff8e4|file:///Users/jerry/.vim/vimrc|129|7|v|vim
	solarized_termcolors
b76a9c1da3050778cb6053683fec1de5|file:///Users/jerry/dotfiles/nvim/.vim/vimrc|72|18|v|vim
	janko-m/vim-test
e3e905b396d9e1cca2a5b562010e85c9|file:///Users/jerry/dotfiles/nvim/.vim/vimrc|62|18|v|vim
	andys8/vscode-jest-snippets
e15ca55c9b805a9f41ba5b9c05630aab|file:///Users/jerry/dotfiles/nvim/.vim/vimrc|43|18|v|vim
	tpope/vim-projectionist
6a5d3852725e741a62f806bfc8ae3251|file:///Users/jerry/dotfiles/nvim/.vim/vimrc|47|3|V|vim
	  " call dein#add('alfredodeza/pytest.vim')
ddc211282b6d55d99d4531fcea6cb01c|file:///Users/jerry/dotfiles/nvim/.vim/vimrc|56|3|V|vim
	  " call dein#add('lifepillar/vim-solarized8')
9b16b32c42fc6fe07e070cd5207afbca|file:///Users/jerry/dotfiles/nvim/.vim/vimrc|60|3|V|vim
	  " call dein#add('andys8/vscode-jest-snippets')
bb9475b9eddd866b51374e630203d6a9|file:///Users/jerry/dotfiles/nvim/.vim/vimrc|69|3|V|vim
	  " call dein#add('janko-m/vim-test')
afe95c585f7021049fd3fa0ef133afa2|file:///Users/jerry/dotfiles/nvim/.vim/vimrc|7|1|V|vim
	let s:deintoml = '~/.vim/config/dein.toml'
82aa98ae1bba55c324c42367a7f7dc45|file:///Users/jerry/dotfiles/nvim/.vim/vimrc|8|1|V|vim
	let s:lazytoml = '~/.vim/config/lazy.toml'
ccd14bf180c17c68276627f047306dd4|file:///Users/jerry/dotfiles/nvim/.vim/vimrc|10|1|V|vim
	let mapleader=','
6a31b094ce5119fc1e98299dd69142b6|file:///Users/jerry/dotfiles/nvim/.vim/config/dein.toml|2|1|V|toml
	repo = 'mhinz/vim-startify'
b3d3cab379ba279ec4a4e9514bd14d3a|file:///Users/jerry/dotfiles/nvim/.vim/config/dein.toml|4|1|V|toml
	  let g:startify_change_to_vcs_root = 1
60374c0fbcbedaf175d7aff397ba175d|file:///Users/jerry/dotfiles/nvim/.vim/config/mapping.vim|1|1|V|vim
	nmap <Leader>ev :e $MYVIMRC<cr>
c0ff3e5a9f84b45839fa4222b68c92f5|list:/extensions|3|1|V|list
	* coc-yank	1.1.2	~/dotfiles/nvim/.config/coc/extensions/node_modules/coc-yank
	* coc-pairs	1.2.16	~/dotfiles/nvim/.config/coc/extensions/node_modules/coc-pairs
	* coc-prettier	1.1.5	~/dotfiles/nvim/.config/coc/extensions/node_modules/coc-prettier
	* coc-snippets	2.1.10	~/dotfiles/nvim/.config/coc/extensions/node_modules/coc-snippets
	+ coc-json	1.2.3	~/dotfiles/nvim/.config/coc/extensions/node_modules/coc-json
	+ coc-java	1.4.4	~/dotfiles/nvim/.config/coc/extensions/node_modules/coc-java
	+ coc-svg	0.0.13	~/dotfiles/nvim/.config/coc/extensions/node_modules/coc-svg
	+ coc-tslint-plugin	1.1.1	~/dotfiles/nvim/.config/coc/extensions/node_modules/coc-tslint-plugin
	+ coc-tsserver	1.3.15	~/dotfiles/nvim/.config/coc/extensions/node_modules/coc-tsserver
	+ coc-vetur	1.1.3	~/dotfiles/nvim/.config/coc/extensions/node_modules/coc-vetur
	+ coc-html	1.2.1	~/dotfiles/nvim/.config/coc/extensions/node_modules/coc-html
	+ coc-yaml	1.0.2	~/dotfiles/nvim/.config/coc/extensions/node_modules/coc-yaml
	+ coc-css	1.2.2	~/dotfiles/nvim/.config/coc/extensions/node_modules/coc-css
	+ coc-angular	0.1.0	~/dotfiles/nvim/.config/coc/extensions/node_modules/coc-angular
	+ coc-python	1.2.5	~/dotfiles/nvim/.config/coc/extensions/node_modules/coc-python
e9051b9d79950b5db9cd717cbdcb8a06|list:/extensions|16|1|V|list
	+ coc-java	1.4.4	~/dotfiles/nvim/.config/coc/extensions/node_modules/coc-java
	+ coc-json	1.2.3	~/dotfiles/nvim/.config/coc/extensions/node_modules/coc-json
26e10c18e8029666ee8b7ec29ad40b9e|list:/yank|67|1|V|list
	line  + coc-java	1.4.4	~/dotfiles/nvim/.config/coc/extensions/node_modules/coc-java + coc-json	1.2.3	~/dotfiles/nvim/.con...
	line  * coc-yank	1.1.2	~/dotfiles/nvim/.config/coc/extensions/node_modules/coc-yank * coc-pairs	1.2.16	~/dotfiles/nvim/.c...
	line  nmap <Leader>ev :e $MYVIMRC<cr>
	line    let g:startify_change_to_vcs_root = 1
	line  repo = 'mhinz/vim-startify'
	line  let mapleader=','
	line  let s:lazytoml = '~/.vim/config/lazy.toml'
	line  let s:deintoml = '~/.vim/config/dein.toml'
	line    " call dein#add('janko-m/vim-test')
	line    " call dein#add('andys8/vscode-jest-snippets')
	line    " call dein#add('lifepillar/vim-solarized8')
	line    " call dein#add('alfredodeza/pytest.vim')
	char  tpope/vim-projectionist
	char  andys8/vscode-jest-snippets
	char  janko-m/vim-test
	char  solarized_termcolors
	char  lifepillar/vim-solarized8
	char  ConradIrwin/vim-bracketed-paste
	char  justinmk/vim-sneak
	line                            "--enable-luainterp",
	char  python_host_prog  
	line  let g:keysound_enable = 1
	line  let g:keysound_py_version = 3 let g:keysound_volume = 500
	line  let g:keysound_volume = 500
	line  let g:keysound_py_version = 3
	char  default
	char  source ~/.vim/config/terminal.vim
	char  kassio/neoterm
	line  [[plugins]] repo = 'kassio/neoterm' hook_add = 'source ~/.vim/config/terminal.vim'
	char  /Users/chen/Documents/GPS/customer/CPE/Goldcorp/data/sample/20180620/Datamine copy
	char  ['tnt']
	char  jmcantrell/vim-virtualenv
	line                  print('there is Au_ppb column')
	char  for target_list
	char  target_list
	char   target_list
	line          
	line  [[plugins]] repo = 'itchyny/vim-cursorword'
	char  honza/vim-snippets
	line  [[plugins]] repo = 'honza/vim-snippets'
	line              self.tem
	line              logger.info('\t3. copy data from tmp table to real table')
	line          if isinstance(table_tmp, collections.abc.Sequence) \                     and not isinstance(table_tmp, str)...
	line      def materialized_view(self):         for table in DRILL_VIEW:             sql='REFRESH MATERIALIZED VIEW CONCUR...
	line              self.update_column_point(self.temp_schema, 'grab_tmp')
	char  kwargs['table_name'] == 'grab_tmp'
	char  kwargs['table_name'] == 'soilindex_tmp'
	line          if 'table_name' in kwargs and kwargs['table_name'] == 'grab_tmp':
	line          if 'table_name' in kwargs and kwargs['table_name'] == 'grab_tmp':             self.update_column_point(self...
	line          if 'table_name' in kwargs and kwargs['table_name'] == 'soilindex_tmp':             self.update_column_point...
	line              self.update_column_point(self.temp_schema, 'soilindex_tmp')
	line      def post_copy_tmp_to_real(self):
	line      def pre_copy_tmp_to_real(self):
	line          elif isinstance(table_real, collections.abc.Sequence) \                     and not isinstance(table_real, ...
	line          elif isinstance(table_real, collections.abc.Sequence) \
	line      def __copy_tmp2real(self,schema,table_tmp):         '''copy data from tmp table to real table          Keyword ...
	line          try:             yield         except Exception as e:             traceback.print_exc()             self.pg...
	line          try:             self.pgsql.connect_uri(self.conf.get_dburi())             yield         except Exception a...
	line      def drop_fk(self,schema,table_tmp):         '''drop the foreign key in real table          Keyword arguments:  ...
	line      def add_fk(self,target_schema,target_table,target_column,refer_schema,refer_table,refer_column):         '''add...
	line  import script.common.log_handler as log
	line          logger.info(sql)
	line   def singleton(cls):     ''' singleton decorator      to make sure there is only one instance of the decorated clas...
	line  #!/usr/bin/env python3 # -*- coding: utf-8 -*-
	line              self.pgsql.query(sql, reply=False)
	line      def update_collar(self):         sql = 'update drill_temp.collar_tmp set point=ST_SETSRID(ST_makepoint(x,y,z),3...
	line      def pre_process_pandas_data(self):
	line      def post_process_table_data(self):
	line              logger.info('-----------------------------------------------------')
	line          logger.info('\t1.2 load the data in raw format, without using pandas')
	line              logger.info('2.2 the table name is {}'.format(table))
	line          logger.info('\t1.5 check if the loaded data is empty or not')
	line  logger=log.get_logger("FileStoreHandler")
	line      def load(self):         print('0.1 do some check to make sure the load process could proceed')
	line          print('0.1 do some check to make sure the load process could proceed')
	line      def step_0_tasks(self):
	line          '''implement this method to process something before moving on          '''
	char  pre_process_pandas_data
	line          raise NotImplementedError
	char  table_names, table_columns, project_id, seq_number, num_split
	line          project_code = self.get_project_code_from_file_name()         print('project code is {}'.format(project_cod...
	line          project_code = self.get_project_code_from_file_name()         print('project code is {}'.format(project_cod...
	line          project_code = self.get_project_code_from_file_name()         print('project code is {}'.format(project_cod...
	line          project_code = self.get_project_code_from_file_name()         print('project code is {}'.format(project_cod...
	line          project_code = self.get_project_code_from_file_name()         print('project code is {}'.format(project_cod...
	line          project_code = self.get_project_code_from_file_name()         print('project code is {}'.format(project_cod...
	line          project_code = self.get_project_code_from_file_name()         print('project code is {}'.format(project_cod...
	line          project_code = self.get_project_code_from_file_name()         print('project code is {}'.format(project_cod...
	line          project_code = self.get_project_code_from_file_name()         print('project code is {}'.format(project_cod...
	line          project_code = self.get_project_code_from_file_name()         print('project code is {}'.format(project_cod...
	line          project_code = self.get_project_code_from_file_name()         print('project code is {}'.format(project_cod...
	line          project_code = self.get_project_code_from_file_name()         print('project code is {}'.format(project_cod...
	line          project_code = self.get_project_code_from_file_name()         print('project code is {}'.format(project_cod...
	line          project_code = self.get_project_code_from_file_name()         print('project code is {}'.format(project_cod...
	line          project_code = self.get_project_code_from_file_name()         print('project code is {}'.format(project_cod...
	line          project_code = self.get_project_code_from_file_name()         print('project code is {}'.format(project_cod...
	line          project_code = self.get_project_code_from_file_name()         print('project code is {}'.format(project_cod...
	line          project_code = self.get_project_code_from_file_name()
	line          elif index_table_name.find('grab')>=0 and col.find('point')>=0:             self.pandas_data[col] = np.arra...
	line                  table_data['chemical_element'] = 'Au'
	line      def add_missing_columns(self, table_name, table_data):
	char  add_missing_columns
	line                  fileCols.append(col)
	line                          fileCols.append(fileCol)
	line          elif index_table_name.find('collar')>=0 and col.find('point')>=0:             self.pandas_data[col] = np.ar...
	line                          if date_column in list(self.pandasData.columns.values):
	line      def unify_datetype_ymd(self,tableData,date_column):         tableData[date_column]=tableData[date_column].filln...
	line          Keyword arguments:             col -- the column name             order - the order, usually 0 - index, 1 -...
	line          Keyword arguments:             panda_table_data -- the pandas data frame you want to filter null
	char  seq_number 
	line          Keyword arguments:             table_data -- the data your want to insert             sql_header - the head...
	line      def genHiddenCol(self,col,order):         self.fileCols.append(col)         # Hidden column can be vary from ea...
	line      def genHiddenCol(self,col,order):         self.fileCols.append(col)         # Hidden column can be vary from ea...
	line      def genHiddenCol(self,col,order):         self.fileCols.append(col)         # Hidden column can be vary from ea...
	line      def genHiddenCol(self,col,order):         self.fileCols.append(col)         # Hidden column can be vary from ea...
	line      def genHiddenCol(self,col,order):         self.fileCols.append(col)         # Hidden column can be vary from ea...
	line      def genHiddenCol(self,col,order):         self.fileCols.append(col)         # Hidden column can be vary from ea...
	line      def genHiddenCol(self,col,order):         self.fileCols.append(col)         # Hidden column can be vary from ea...
	line      def genHiddenCol(self,col,order):         self.fileCols.append(col)         # Hidden column can be vary from ea...
	line      def genHiddenCol(self,col,order):         self.fileCols.append(col)         # Hidden column can be vary from ea...
	line      def genHiddenCol(self,col,order):         self.fileCols.append(col)         # Hidden column can be vary from ea...
	line      def genHiddenCol(self,col,order):         self.fileCols.append(col)         # Hidden column can be vary from ea...
	line      def genHiddenCol(self,col,order):         self.fileCols.append(col)         # Hidden column can be vary from ea...
	line      def genHiddenCol(self,col,order):         self.fileCols.append(col)         # Hidden column can be vary from ea...
	line      def genHiddenCol(self,col,order):         self.fileCols.append(col)         # Hidden column can be vary from ea...
	line      def genHiddenCol(self,col,order):         self.fileCols.append(col)         # Hidden column can be vary from ea...
	line      def genHiddenCol(self,col,order):         self.fileCols.append(col)         # Hidden column can be vary from ea...
	line      def genHiddenCol(self,col,order):         self.fileCols.append(col)         # Hidden column can be vary from ea...
	line      def genHiddenCol(self,col,order):         self.fileCols.append(col)         # Hidden column can be vary from ea...
	line      def genHiddenCol(self,col,order):         self.fileCols.append(col)         # Hidden column can be vary from ea...
	line      def genHiddenCol(self,col,order):         self.fileCols.append(col)         # Hidden column can be vary from ea...
	line      def genHiddenCol(self,col,order):         self.fileCols.append(col)         # Hidden column can be vary from ea...
	line      def genHiddenCol(self,col,order):         self.fileCols.append(col)         # Hidden column can be vary from ea...
	line      def genHiddenCol(self,col,order):         self.fileCols.append(col)         # Hidden column can be vary from ea...
	line      def genHiddenCol(self,col,order):         self.fileCols.append(col)         # Hidden column can be vary from ea...
	line      def genHiddenCol(self,col,order):         self.fileCols.append(col)         # Hidden column can be vary from ea...
	line      def genHiddenCol(self,col,order):         self.fileCols.append(col)         # Hidden column can be vary from ea...
	line      def genHiddenCol(self,col,order):         self.fileCols.append(col)         # Hidden column can be vary from ea...
	line      def genHiddenCol(self,col,order):         self.fileCols.append(col)         # Hidden column can be vary from ea...
	line      def genHiddenCol(self,col,order):         self.fileCols.append(col)         # Hidden column can be vary from ea...
	line      def genHiddenCol(self,col,order):         self.fileCols.append(col)
	line      def genHiddenCol(self,col,order):
	line          self.numRawRecords = len(self.pandasData)
	line                  self.pgsql.query(sql,reply=False)
	line          Keyword arguments:             table_names -- the table names you get from configuration, it could
	line      def genSqlValues(self,batchData):         sqlValue = []         for record in batchData.values:             val...
	line  import math
	line          Keyword arguments:             commit -- to commit your query or not, default to True
	line      def insertByBatch(self,tableData,sqlHeader):         # 4.1 divide the records into batches         numRecords =...
	line                  self.sqlHeaders = self.generate_sql_header(table_names, table_columns)
	line      def fetchFromFile(self,table,order,tableColumns):         self.fileCols = []         # 1. judge whether current...
	line              for order in range(0, numSplit):                 print('******************* order=', order, '**********...
	line              print('*******************',table,'************************')             numSplit = self.numSplit[i]  ...
	line              print('*******************',table,'************************')             numSplit = self.numSplit[i]  ...
	line              print('*******************',table,'************************')             numSplit = self.numSplit[i]  ...
	line              print('*******************',table,'************************')             numSplit = self.numSplit[i]
	line              print('*******************',table,'************************')
	line          for i,table in enumerate(self.table):
	line          # 3.Fetch data from data frame for each table         # 3.1 get the split number of each table
	line  		"veinindex":["veindata"],
	char  measurement_value
	char  measurement_unit
	line  		"assaydata":["chemical_element", "measurement_unit", "measurement_value"],
	char  chemical_element
	line      "soildata.chemical_element": [""],
	line      "soilindex.rf_pct": [""],
	line      "soilindex.landuse": [""],
	line      "soilindex.sampleid": [""],
	line          print('project code is {}'.format(project_code))
	char  mineral_count
	char  mineral
	char  "drill_temp.veinindex_tmp"
	line  		"VeinsTable1.csv":["drill_temp.veindata_tmp"],
	line  NECESSARY_FILES_SUBFIX=("AltMin.csv","Assay.csv","Collar.csv","Event.csv",                         "IntervalStructu...
	char  table_name.find('data') >= 0
	line                  errorFileName = str(datetime.datetime.now()) + '_' + \                                 str(self.inp...
	line          Keyword arguments:             panda_table_data -- the pandas data frame you want to remove                ...
	line          Keyword arguments:             table_names -- the table names you get from configuration, it could         ...
	line          Keyword arguments:             schema -- the tmp schema, not the real one             table_tmp -- the tmp ...
	line      def genSqlHeader(self):         self.sqlHeaders = [""] * len(self.table)         i=0         for table in self....
	line      def drop_duplicates(self,tableData):         return tableData.drop_duplicates()
	line          self.pandasData = self.filterNullData(self.pandasData, self.tableColumns[0], self.table[0], order=0)
	line      def matchByConfig(self,table,col,order):         dic_match = self.dic_match         try:             matchedCol...
	line      def filterNullData(self,tableData,tableColumns,table,order):         dic_notnull=self.dic_notnull         null_...
	line      def filterNullData(self,tableData,tableColumns,table,order):         dic_notnull=self.dic_notnull         null_...
	line      def filterNullData(self,tableData,tableColumns,table,order):         dic_notnull=self.dic_notnull         null_...
	line      def filterNullData(self,tableData,tableColumns,table,order):         dic_notnull=self.dic_notnull         null_...
	line      def filterNullData(self,tableData,tableColumns,table,order):         dic_notnull=self.dic_notnull         null_...
	line      def filterNullData(self,tableData,tableColumns,table,order):         dic_notnull=self.dic_notnull         null_...
	line      def filterNullData(self,tableData,tableColumns,table,order):         dic_notnull=self.dic_notnull         null_...
	line      def filterNullData(self,tableData,tableColumns,table,order):         dic_notnull=self.dic_notnull         null_...
	line      def filterNullData(self,tableData,tableColumns,table,order):         dic_notnull=self.dic_notnull         null_...
	line      def filterNullData(self,tableData,tableColumns,table,order):         dic_notnull=self.dic_notnull         null_...
	line      def filterNullData(self,tableData,tableColumns,table,order):         dic_notnull=self.dic_notnull         null_...
	line      def filterNullData(self,tableData,tableColumns,table,order):         dic_notnull=self.dic_notnull         null_...
	line      def filterNullData(self,tableData,tableColumns,table,order):         dic_notnull=self.dic_notnull         null_...
	line      def filterNullData(self,tableData,tableColumns,table,order):         dic_notnull=self.dic_notnull         null_...
	line      def filterNullData(self,tableData,tableColumns,table,order):         dic_notnull=self.dic_notnull         null_...
	line      def filterNullData(self,tableData,tableColumns,table,order):         dic_notnull=self.dic_notnull         null_...
	line      def filterNullData(self,tableData,tableColumns,table,order):         dic_notnull=self.dic_notnull         null_...
	line      def filterNullData(self,tableData,tableColumns,table,order):         dic_notnull=self.dic_notnull         null_...
	line      def filterNullData(self,tableData,tableColumns,table,order):         dic_notnull=self.dic_notnull         null_...
	line      def filterNullData(self,tableData,tableColumns,table,order):         dic_notnull=self.dic_notnull         null_...
	line      def filterNullData(self,tableData,tableColumns,table,order):         dic_notnull=self.dic_notnull         null_...
	line      def filterNullData(self,tableData,tableColumns,table,order):         dic_notnull=self.dic_notnull         null_...
	line      def filterNullData(self,tableData,tableColumns,table,order):         dic_notnull=self.dic_notnull         null_...
	line      def filterNullData(self,tableData,tableColumns,table,order):         dic_notnull=self.dic_notnull         null_...
	line      def filterNullData(self,tableData,tableColumns,table,order):         dic_notnull=self.dic_notnull         null_...
	line      def filterNullData(self,tableData,tableColumns,table,order):         dic_notnull=self.dic_notnull         null_...
	line      def filterNullData(self,tableData,tableColumns,table,order):         dic_notnull=self.dic_notnull         null_...
	line      def filterNullData(self,tableData,tableColumns,table,order):         dic_notnull=self.dic_notnull         null_...
	line      def filterNullData(self,tableData,tableColumns,table,order):         dic_notnull=self.dic_notnull         null_...
	line      def filterNullData(self,tableData,tableColumns,table,order):         dic_notnull=self.dic_notnull         null_...
	line      def filterNullData(self,tableData,tableColumns,table,order):         dic_notnull=self.dic_notnull         null_...
	line      def filterNullData(self,tableData,tableColumns,table,order):         dic_notnull=self.dic_notnull         null_...
	line      def filterNullData(self,tableData,tableColumns,table,order):         dic_notnull=self.dic_notnull         null_...
	line      def filterNullData(self,tableData,tableColumns,table,order):         dic_notnull=self.dic_notnull         null_...
	line      def filterNullData(self,tableData,tableColumns,table,order):         dic_notnull=self.dic_notnull
	line      def filterNullData(self,tableData,tableColumns,table,order):
	char  notNullColumns
	char  dic_notnull
	char  dic_match 
	line          lines=[]         with open(self.filePath,'r',encoding=self.encoding) as file:             #  lines = list(c...
	line      def detectErrorFormatRecords(self,rawData):         i=0         j=0         numCol=len(self.pandasData.columns)...
	char  error_records_filepath 
	char  error_records_filepath
	line      def logErrorRecord(self,errorRecord,errorFileName):         errorFilePath=self.error_records_filepath+errorFile...
	line      def detectErrorFormatRecords(self,rawData):         i=0         j=0         numCol=len(self.pandasData.columns)...
	line      def detectErrorFormatRecords(self,rawData):         i=0         j=0         numCol=len(self.pandasData.columns)...
	line      def detectErrorFormatRecords(self,rawData):         i=0         j=0         numCol=len(self.pandasData.columns)...
	line      def detectErrorFormatRecords(self,rawData):         i=0         j=0         numCol=len(self.pandasData.columns)...
	line      def detectErrorFormatRecords(self,rawData):         i=0         j=0         numCol=len(self.pandasData.columns)...
	line      def detectErrorFormatRecords(self,rawData):         i=0         j=0         numCol=len(self.pandasData.columns)...
	line      def detectErrorFormatRecords(self,rawData):         i=0         j=0         numCol=len(self.pandasData.columns)...
	line      def detectErrorFormatRecords(self,rawData):         i=0         j=0         numCol=len(self.pandasData.columns)...
	line      def detectErrorFormatRecords(self,rawData):         i=0         j=0         numCol=len(self.pandasData.columns)...
	line      def detectErrorFormatRecords(self,rawData):         i=0         j=0         numCol=len(self.pandasData.columns)...
	line      def detectErrorFormatRecords(self,rawData):         i=0         j=0         numCol=len(self.pandasData.columns)...
	line      def detectErrorFormatRecords(self,rawData):         i=0         j=0         numCol=len(self.pandasData.columns)...
	line      def detectErrorFormatRecords(self,rawData):         i=0         j=0         numCol=len(self.pandasData.columns)...
	line      def detectErrorFormatRecords(self,rawData):         i=0         j=0         numCol=len(self.pandasData.columns)...
	line      def detectErrorFormatRecords(self,rawData):         i=0         j=0         numCol=len(self.pandasData.columns)...
	line      def detectErrorFormatRecords(self,rawData):         i=0         j=0         numCol=len(self.pandasData.columns)...
	line      def detectErrorFormatRecords(self,rawData):         i=0         j=0         numCol=len(self.pandasData.columns)...
	line      def detectErrorFormatRecords(self,rawData):         i=0         j=0         numCol=len(self.pandasData.columns)...
	line      def detectErrorFormatRecords(self,rawData):         i=0         j=0         numCol=len(self.pandasData.columns)...
	line      def detectErrorFormatRecords(self,rawData):         i=0         j=0         numCol=len(self.pandasData.columns)...
	line      def detectErrorFormatRecords(self,rawData):         i=0         j=0         numCol=len(self.pandasData.columns)...
	line      def detectErrorFormatRecords(self,rawData):         i=0         j=0         numCol=len(self.pandasData.columns)...
	line      def detectErrorFormatRecords(self,rawData):         i=0         j=0         numCol=len(self.pandasData.columns)...
	line      def detectErrorFormatRecords(self,rawData):         i=0         j=0         numCol=len(self.pandasData.columns)...
	line      def detectErrorFormatRecords(self,rawData):         i=0         j=0         numCol=len(self.pandasData.columns)...
	line      def detectErrorFormatRecords(self,rawData):         i=0         j=0         numCol=len(self.pandasData.columns)...
	line      def detectErrorFormatRecords(self,rawData):         i=0         j=0         numCol=len(self.pandasData.columns)...
	line      def detectErrorFormatRecords(self,rawData):         i=0         j=0         numCol=len(self.pandasData.columns)...
	line      def detectErrorFormatRecords(self,rawData):         i=0         j=0         numCol=len(self.pandasData.columns)...
	line      def detectErrorFormatRecords(self,rawData):         i=0         j=0         numCol=len(self.pandasData.columns)...
	line      def detectErrorFormatRecords(self,rawData):         i=0         j=0         numCol=len(self.pandasData.columns)...
	line      def detectErrorFormatRecords(self,rawData):         i=0         j=0         numCol=len(self.pandasData.columns)
	line      def detectErrorFormatRecords(self,rawData):         i=0         j=0
	line      def detectErrorFormatRecords(self,rawData):         i=0
	line      def detectErrorFormatRecords(self,rawData):
	line          self.batchSize=batchSize
	line          self.encoding='cp1252'#old encoding='cp437',current encoding='cp1252'
	char  batchSize=60000, fileDelimiter=','
	line  import csv
	line  import numpy as np import pandas as pd
	line          self.pandasData=pd.read_csv(self.filePath,                 quotechar='"',                 delimiter=self.fi...
	line      "grab.minz_2_pct": [""],
	line      "grab.minz_1": [""],     "grab.minz_1_pct": [""],
	line      "grab.minz_1": [""],
	line      "grab.alt_2_deg": [""],
	line      "grab.alt_1_deg": [""],
	line      "grab.alt_1": [""],
	line      "grab.weathering": [""],
	line      "grab.foliation": [""],
	line      "grab.grain_size": [""],
	line      "grab.grab_type": [""],
	line      "grab.colour": [""],
	line      "grab.lithology": [""],
	line      "grab.au_ppm": [""],
	line      "grab.z": [""],
	line      "grab.y": [""],
	line      "grab.x": [""],
	line      "grab.lab_reference": [""],
	line      "grab.analys_date": [""],
	line      "grab.medium_code": [""],
	line      "grab.sampleid": [""],
	line  conf_dev = config_handler.get_config(         os.path.join( os.path.dirname(__file__), '../../config/config_dev.jso...
	line          self.dic_match = conf_dev['Dic_drill']
	line          self.error_records_filepath='../drill/error_records/'
	line  import script.common.cos_handler as cos_handler import script.common.gip_exception as gip_exception
	line  import datetime
	line  import traceback
	line      def __check_bucket(self, coshandler, bucket):         if not coshandler.check_bucket(bucket):             try: ...
	line      def check_all_bucket(self,coshandler):         bucket_list=[self.target_bucket,self.error_bucket,self.backup_bu...
	line          self.pgsql.query(cmd=sql, reply=False)
	line              col = 'holeid,projectid,fromid,toid,'+ dic_intervalid[table_1]
	char  pointstructure_index,
	line  import script.common.cos_handler as cos_handler
	line          try:             if self.file_on_ObjectStorage:                 print("get access to object storage")      ...
	line              #  for file in upload_files:                 #  if file =='.DS_Store' or (not file.endswith(('.txt', '....
	line  import sys sys.path.append(path.dirname( path.dirname( path.dirname( path.abspath(__file__) ))) )
	line  if __name__ == '__main__':     conf = config_handler.ConfigHandler()
	line          print(upload_files)
	line          print(upload_files)         logger.info("verify files to be loaded into db ")
	line              #  print("\t2. download files and load into tmp table")
	char  DATA_TABLE
	line          super().clean_tmp_tables(self.tmp_schema, INDEX_TABLE)
	char  INDEX_TABLE
	line      def __clean_table_commit(self,schema,table):         sql='delete from {0}.{1} where 1=1'.format(schema,table)  ...
	line      @contextmanager     def run_query_context(self, commit=True):         '''the context manager to run a query    ...
	line      @contextmanager     def run_query_context(self, commit=True):         '''the context manager to run a query    ...
	line      @contextmanager     def run_query_context(self, commit=True):         '''the context manager to run a query    ...
	line      @contextmanager     def run_query_context(self, commit=True):         '''the context manager to run a query    ...
	line      @contextmanager     def run_query_context(self, commit=True):         '''the context manager to run a query    ...
	line      @contextmanager     def run_query_context(self, commit=True):         '''the context manager to run a query    ...
	line      @contextmanager     def run_query_context(self, commit=True):         '''the context manager to run a query    ...
	line      @contextmanager     def run_query_context(self, commit=True):         '''the context manager to run a query    ...
	line      @contextmanager     def run_query_context(self, commit=True):         '''the context manager to run a query    ...
	line      @contextmanager     def run_query_context(self, commit=True):         '''the context manager to run a query    ...
	line      @contextmanager     def run_query_context(self, commit=True):         '''the context manager to run a query    ...
	line      @contextmanager     def run_query_context(self, commit=True):         '''the context manager to run a query    ...
	line      @contextmanager     def run_query_context(self, commit=True):         '''the context manager to run a query    ...
	line      @contextmanager     def run_query_context(self, commit=True):         '''the context manager to run a query    ...
	line      @contextmanager     def run_query_context(self, commit=True):         '''the context manager to run a query    ...
	line      @contextmanager     def run_query_context(self, commit=True):         '''the context manager to run a query 
	line      @contextmanager     def run_query_context(self, commit=True):         '''the context manager to run a query
	line      @contextmanager     def run_query_context(self, commit=True):
	line      @contextmanager
	line              print("\t1. clean up all tmp table and local files")
031278dae3c064a18ee33ca93f9f750e|list:/yank|332|1|V|list
	char  janko-m/vim-test
	char  justinmk/vim-sneak
	char  jmcantrell/vim-virtualenv
	char  andys8/vscode-jest-snippets
	line    " call dein#add('andys8/vscode-jest-snippets')
	line      def detectErrorFormatRecords(self,rawData):         i=0         j=0
	line      def detectErrorFormatRecords(self,rawData):         i=0         j=0         numCol=len(self.pandasData.columns)
	line      def fetchFromFile(self,table,order,tableColumns):         self.fileCols = []         # 1. judge whether current...
	line  + coc-java	1.4.4	~/dotfiles/nvim/.config/coc/extensions/node_modules/coc-java + coc-json	1.2.3	~/dotfiles/nvim/.con...
	line      def detectErrorFormatRecords(self,rawData):         i=0         j=0         numCol=len(self.pandasData.columns)...
	line      def detectErrorFormatRecords(self,rawData):         i=0         j=0         numCol=len(self.pandasData.columns)...
	line      def detectErrorFormatRecords(self,rawData):         i=0         j=0         numCol=len(self.pandasData.columns)...
	line      def detectErrorFormatRecords(self,rawData):         i=0         j=0         numCol=len(self.pandasData.columns)...
	line      def detectErrorFormatRecords(self,rawData):         i=0         j=0         numCol=len(self.pandasData.columns)...
	line      def detectErrorFormatRecords(self,rawData):         i=0         j=0         numCol=len(self.pandasData.columns)...
	line      def detectErrorFormatRecords(self,rawData):         i=0         j=0         numCol=len(self.pandasData.columns)...
	line      def detectErrorFormatRecords(self,rawData):         i=0         j=0         numCol=len(self.pandasData.columns)...
	line      def detectErrorFormatRecords(self,rawData):         i=0         j=0         numCol=len(self.pandasData.columns)...
	line      def detectErrorFormatRecords(self,rawData):         i=0         j=0         numCol=len(self.pandasData.columns)...
	line      def detectErrorFormatRecords(self,rawData):         i=0         j=0         numCol=len(self.pandasData.columns)...
	line      def detectErrorFormatRecords(self,rawData):         i=0         j=0         numCol=len(self.pandasData.columns)...
	line      def detectErrorFormatRecords(self,rawData):         i=0         j=0         numCol=len(self.pandasData.columns)...
	line      def detectErrorFormatRecords(self,rawData):         i=0         j=0         numCol=len(self.pandasData.columns)...
	line      def detectErrorFormatRecords(self,rawData):         i=0         j=0         numCol=len(self.pandasData.columns)...
	line      def detectErrorFormatRecords(self,rawData):         i=0         j=0         numCol=len(self.pandasData.columns)...
	line      def detectErrorFormatRecords(self,rawData):         i=0         j=0         numCol=len(self.pandasData.columns)...
	line      def detectErrorFormatRecords(self,rawData):         i=0         j=0         numCol=len(self.pandasData.columns)...
	line      def detectErrorFormatRecords(self,rawData):         i=0         j=0         numCol=len(self.pandasData.columns)...
	line      def detectErrorFormatRecords(self,rawData):         i=0         j=0         numCol=len(self.pandasData.columns)...
	line      def detectErrorFormatRecords(self,rawData):         i=0         j=0         numCol=len(self.pandasData.columns)...
	line      def detectErrorFormatRecords(self,rawData):         i=0         j=0         numCol=len(self.pandasData.columns)...
	line      def detectErrorFormatRecords(self,rawData):         i=0         j=0         numCol=len(self.pandasData.columns)...
	line      def detectErrorFormatRecords(self,rawData):         i=0         j=0         numCol=len(self.pandasData.columns)...
	line      def detectErrorFormatRecords(self,rawData):         i=0         j=0         numCol=len(self.pandasData.columns)...
	line      def detectErrorFormatRecords(self,rawData):         i=0         j=0         numCol=len(self.pandasData.columns)...
	line      def detectErrorFormatRecords(self,rawData):         i=0         j=0         numCol=len(self.pandasData.columns)...
	line      def detectErrorFormatRecords(self,rawData):         i=0         j=0         numCol=len(self.pandasData.columns)...
	line      def detectErrorFormatRecords(self,rawData):         i=0         j=0         numCol=len(self.pandasData.columns)...
	line      def detectErrorFormatRecords(self,rawData):         i=0         j=0         numCol=len(self.pandasData.columns)...
	line      def detectErrorFormatRecords(self,rawData):         i=0         j=0         numCol=len(self.pandasData.columns)...
	line      def detectErrorFormatRecords(self,rawData):         i=0         j=0         numCol=len(self.pandasData.columns)...
	line  conf_dev = config_handler.get_config(         os.path.join( os.path.dirname(__file__), '../../config/config_dev.jso...
	char  tpope/vim-projectionist
	line    " call dein#add('janko-m/vim-test')
	line          print('project code is {}'.format(project_code))
	char  table_names, table_columns, project_id, seq_number, num_split
	line          project_code = self.get_project_code_from_file_name()
	line              col = 'holeid,projectid,fromid,toid,'+ dic_intervalid[table_1]
	line          project_code = self.get_project_code_from_file_name()         print('project code is {}'.format(project_cod...
	line          project_code = self.get_project_code_from_file_name()         print('project code is {}'.format(project_cod...
	line          project_code = self.get_project_code_from_file_name()         print('project code is {}'.format(project_cod...
	line          project_code = self.get_project_code_from_file_name()         print('project code is {}'.format(project_cod...
	line          project_code = self.get_project_code_from_file_name()         print('project code is {}'.format(project_cod...
	line          project_code = self.get_project_code_from_file_name()         print('project code is {}'.format(project_cod...
	line          project_code = self.get_project_code_from_file_name()         print('project code is {}'.format(project_cod...
	line          project_code = self.get_project_code_from_file_name()         print('project code is {}'.format(project_cod...
	line          project_code = self.get_project_code_from_file_name()         print('project code is {}'.format(project_cod...
	line          project_code = self.get_project_code_from_file_name()         print('project code is {}'.format(project_cod...
	line          project_code = self.get_project_code_from_file_name()         print('project code is {}'.format(project_cod...
	line          project_code = self.get_project_code_from_file_name()         print('project code is {}'.format(project_cod...
	line          project_code = self.get_project_code_from_file_name()         print('project code is {}'.format(project_cod...
	line          project_code = self.get_project_code_from_file_name()         print('project code is {}'.format(project_cod...
	line          project_code = self.get_project_code_from_file_name()         print('project code is {}'.format(project_cod...
	line          project_code = self.get_project_code_from_file_name()         print('project code is {}'.format(project_cod...
	line          project_code = self.get_project_code_from_file_name()         print('project code is {}'.format(project_cod...
	line          try:             if self.file_on_ObjectStorage:                 print("get access to object storage")      ...
8bbf6f177092074b0a2b7cc143d2acf1|list:/yank|19|1|V|list
	line  + coc-java	1.4.4	~/dotfiles/nvim/.config/coc/extensions/node_modules/coc-java + coc-json	1.2.3	~/dotfiles/nvim/.con...
	line  conf_dev = config_handler.get_config(         os.path.join( os.path.dirname(__file__), '../../config/config_dev.jso...
	line          print('project code is {}'.format(project_code))
	line          project_code = self.get_project_code_from_file_name()
	line          project_code = self.get_project_code_from_file_name()         print('project code is {}'.format(project_cod...
	line          project_code = self.get_project_code_from_file_name()         print('project code is {}'.format(project_cod...
	line          project_code = self.get_project_code_from_file_name()         print('project code is {}'.format(project_cod...
	line          project_code = self.get_project_code_from_file_name()         print('project code is {}'.format(project_cod...
	line          project_code = self.get_project_code_from_file_name()         print('project code is {}'.format(project_cod...
	line          project_code = self.get_project_code_from_file_name()         print('project code is {}'.format(project_cod...
	line          project_code = self.get_project_code_from_file_name()         print('project code is {}'.format(project_cod...
	line          project_code = self.get_project_code_from_file_name()         print('project code is {}'.format(project_cod...
	line          project_code = self.get_project_code_from_file_name()         print('project code is {}'.format(project_cod...
	line          project_code = self.get_project_code_from_file_name()         print('project code is {}'.format(project_cod...
	line          project_code = self.get_project_code_from_file_name()         print('project code is {}'.format(project_cod...
	line          project_code = self.get_project_code_from_file_name()         print('project code is {}'.format(project_cod...
	line          project_code = self.get_project_code_from_file_name()         print('project code is {}'.format(project_cod...
	line          project_code = self.get_project_code_from_file_name()         print('project code is {}'.format(project_cod...
	line          project_code = self.get_project_code_from_file_name()         print('project code is {}'.format(project_cod...
	line          project_code = self.get_project_code_from_file_name()         print('project code is {}'.format(project_cod...
	line          project_code = self.get_project_code_from_file_name()         print('project code is {}'.format(project_cod...
	line          try:             if self.file_on_ObjectStorage:                 print("get access to object storage")      ...
b4a33a327f36fea0548623a22430a69c|list:/yank|23|1|V|list
	line  + coc-java	1.4.4	~/dotfiles/nvim/.config/coc/extensions/node_modules/coc-java + coc-json	1.2.3	~/dotfiles/nvim/.con...
	line          project_code = self.get_project_code_from_file_name()         print('project code is {}'.format(project_cod...
	line          project_code = self.get_project_code_from_file_name()         print('project code is {}'.format(project_cod...
	line          project_code = self.get_project_code_from_file_name()         print('project code is {}'.format(project_cod...
	line          project_code = self.get_project_code_from_file_name()         print('project code is {}'.format(project_cod...
	line          project_code = self.get_project_code_from_file_name()         print('project code is {}'.format(project_cod...
	line          project_code = self.get_project_code_from_file_name()         print('project code is {}'.format(project_cod...
	line          project_code = self.get_project_code_from_file_name()         print('project code is {}'.format(project_cod...
	line          project_code = self.get_project_code_from_file_name()         print('project code is {}'.format(project_cod...
	line          project_code = self.get_project_code_from_file_name()         print('project code is {}'.format(project_cod...
	line          project_code = self.get_project_code_from_file_name()         print('project code is {}'.format(project_cod...
	line          project_code = self.get_project_code_from_file_name()         print('project code is {}'.format(project_cod...
	line          project_code = self.get_project_code_from_file_name()         print('project code is {}'.format(project_cod...
	line          project_code = self.get_project_code_from_file_name()         print('project code is {}'.format(project_cod...
	line          project_code = self.get_project_code_from_file_name()         print('project code is {}'.format(project_cod...
	line          project_code = self.get_project_code_from_file_name()         print('project code is {}'.format(project_cod...
	line          project_code = self.get_project_code_from_file_name()         print('project code is {}'.format(project_cod...
	line          project_code = self.get_project_code_from_file_name()         print('project code is {}'.format(project_cod...
38297e5f5db645721607d265e804f063|file:///Users/jerry/dotfiles/nvim/.vim/vimrc|1|1|V|vim
	if &compatible
85e24cd4dbebceac16cb92dd8cbfd6d0|file:///Users/jerry/dotfiles/nvim/.vim/vimrc|5|1|V|vim
	set runtimepath+=~/.cache/dein/repos/github.com/Shougo/dein.vim
